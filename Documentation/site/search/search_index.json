{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"AWS IoT Core Configuration: 'AWS_IoT.md' Raspberry Pi with Humidity and Temperature Sensor Configuration: 'RaPi_with_Sensor.md'","title":"Home"},{"location":"AWS_IoT/","text":"Appendix - 1: AWS IoT Platform Configuration 1. What is AWS IoT Platform? AWS IoT provides broad and deep functionality, spanning the edge to the cloud, so you can build IoT solutions for virtually any use case across a wide range of devices. Since AWS IoT integrates with AI services, you can make devices smarter, even without Internet connectivity. Built on the AWS cloud, used by millions of customers in 190 countries, AWS IoT can easily scale as your device fleet grows and your business requirements evolve. AWS IoT also offers the most comprehensive security features so you can create preventative security policies and respond immediately to potential security issues. 2. What Config Do We Need for this Lab? For this lab we are going to use the MQTT Broker service from AWS IoT Core, which would be used by the IoT Sensor devices (Raspberry Pi with DHT22 sensor in our case) to publish the sensor data. As the direct MQTT access is not permitted by AWS, we would have to create certificate and policies to allow our sensor devices to publish the sensor data on MQTT Broker. This article would explain the process for the same. 3. Prerequisites: You need to have a valid AWS Account with Administrative access to AWS IoT Core. 4. Create new MQTT Policies: Login to AWS console and open AWS IoT Core page. 4.1 Create New Policies: Click on the \" Create \" button on \" Secure -- Policies \" page to create a new policy as shown in the following screenshot - Note: If you are creating a Policy for the first time, you may see a different screen with \" Create a policy \" button. Just click on that button and follow other steps. 4.2 Create MQTT_Connect Policy: Create a new MQTT_Connect policity with \" iot:Connect \" action, \" * \" Resource ARN and \" Allow \" Effect as shown in the following screenshot - 4.3 Create MQTT_Publish Policy: Create a new MQTT_Publish policity with \" iot:Publish \" action, \" * \" Resource ARN and \" Allow \" Effect as shown in the following screenshot - 4.4 Create MQTT_Subscribe Policy: Create a new MQTT_Subscribe policity with \" iot:Subscribe \" action, \" * \" Resource ARN and \" Allow \" Effect as shown in the following screenshot - 4.4 Check the Policies Page: Your Policies page should look similar to the following screenshot - 5. Create and Download new Certificate: 5.1 Create New Policies: Click on the \" Create \" button on \" Secure -- Certificates \" page to create a new certificate as shown in the following screenshot - Note: If you are creating a Certificate for the first time, you may see a different screen with \" Create a certificate \" button. Just click on that button and follow other steps. 5.2 Create One-Click-Certificate: Click on the \" Create certificate \" button to create a new One-click-certificate as shown in the following screenshot - 5.3 Download Certificates: Click on the \" Download \" buttons and download the Certificate file, Public key, and Private key file as shown in the following screenshot - 5.4 Close the Create Certificate Page: Close the \"Create Certificate\" page by clicking on the \" Done \" button. 5.4 Download Amazon Root CA Certificate: Save the \" RSA 2048 bit key: Amazon Root CA 1 \" file from the following page as \" AmazonRootCA.crt \" - (Page URL - https://docs.aws.amazon.com/iot/latest/developerguide/managing-device-certs.html#server-authentication ) Note: Save all these Certificate and Key files on your machine as you would need to upload them on Raspberry Pi (Appendix-2) and on MQTT_to_DB Agent. 5.5 Check the Certificates Page: Now on the \" Secure -- Certificates \" page you should see a new certificate. Note that your certificate is still inactive. 6. Activate the Certificate: Navigate to \" Secure -- Certificates \"; click on the Certificate options and select \" Activate \" to activate the certificate as shown in following screenshot - 7. Associate the Policies with Certificate: Navigate to \" Secure -- Certificates \" and click on the certificate to modify the certificate properties. On this certificate properties page click on \" Policies \" and from the \" Actions \" menu select \" Attach Policy \" as shown in the following screenshot - Select all the policies and click on \" Attach \" button as shown in the following screenshot - Your certificate Policies section should look like this - 8. Locate Custom Endpoint: You would need Custom endpoint to connect to the AWS IoT platform. It will be used by the MQTT clients as MQTT Broker host. You can locate your Customer Endpoint under \"Settings\" section as shown in the following screenshot -","title":"Appendix-1 AWS IoT Core Configuration"},{"location":"AWS_IoT/#appendix-1-aws-iot-platform-configuration","text":"","title":"Appendix - 1: AWS IoT Platform Configuration"},{"location":"AWS_IoT/#1-what-is-aws-iot-platform","text":"AWS IoT provides broad and deep functionality, spanning the edge to the cloud, so you can build IoT solutions for virtually any use case across a wide range of devices. Since AWS IoT integrates with AI services, you can make devices smarter, even without Internet connectivity. Built on the AWS cloud, used by millions of customers in 190 countries, AWS IoT can easily scale as your device fleet grows and your business requirements evolve. AWS IoT also offers the most comprehensive security features so you can create preventative security policies and respond immediately to potential security issues.","title":"1. What is AWS IoT Platform?"},{"location":"AWS_IoT/#2-what-config-do-we-need-for-this-lab","text":"For this lab we are going to use the MQTT Broker service from AWS IoT Core, which would be used by the IoT Sensor devices (Raspberry Pi with DHT22 sensor in our case) to publish the sensor data. As the direct MQTT access is not permitted by AWS, we would have to create certificate and policies to allow our sensor devices to publish the sensor data on MQTT Broker. This article would explain the process for the same.","title":"2. What Config Do We Need for this Lab?"},{"location":"AWS_IoT/#3-prerequisites","text":"You need to have a valid AWS Account with Administrative access to AWS IoT Core.","title":"3. Prerequisites:"},{"location":"AWS_IoT/#4-create-new-mqtt-policies","text":"Login to AWS console and open AWS IoT Core page.","title":"4. Create new MQTT Policies:"},{"location":"AWS_IoT/#41-create-new-policies","text":"Click on the \" Create \" button on \" Secure -- Policies \" page to create a new policy as shown in the following screenshot - Note: If you are creating a Policy for the first time, you may see a different screen with \" Create a policy \" button. Just click on that button and follow other steps.","title":"4.1 Create New Policies:"},{"location":"AWS_IoT/#42-create-mqtt_connect-policy","text":"Create a new MQTT_Connect policity with \" iot:Connect \" action, \" * \" Resource ARN and \" Allow \" Effect as shown in the following screenshot -","title":"4.2 Create MQTT_Connect Policy:"},{"location":"AWS_IoT/#43-create-mqtt_publish-policy","text":"Create a new MQTT_Publish policity with \" iot:Publish \" action, \" * \" Resource ARN and \" Allow \" Effect as shown in the following screenshot -","title":"4.3 Create MQTT_Publish Policy:"},{"location":"AWS_IoT/#44-create-mqtt_subscribe-policy","text":"Create a new MQTT_Subscribe policity with \" iot:Subscribe \" action, \" * \" Resource ARN and \" Allow \" Effect as shown in the following screenshot -","title":"4.4 Create MQTT_Subscribe Policy:"},{"location":"AWS_IoT/#44-check-the-policies-page","text":"Your Policies page should look similar to the following screenshot -","title":"4.4 Check the Policies Page:"},{"location":"AWS_IoT/#5-create-and-download-new-certificate","text":"","title":"5. Create and Download new Certificate:"},{"location":"AWS_IoT/#51-create-new-policies","text":"Click on the \" Create \" button on \" Secure -- Certificates \" page to create a new certificate as shown in the following screenshot - Note: If you are creating a Certificate for the first time, you may see a different screen with \" Create a certificate \" button. Just click on that button and follow other steps.","title":"5.1 Create New Policies:"},{"location":"AWS_IoT/#52-create-one-click-certificate","text":"Click on the \" Create certificate \" button to create a new One-click-certificate as shown in the following screenshot -","title":"5.2 Create One-Click-Certificate:"},{"location":"AWS_IoT/#53-download-certificates","text":"Click on the \" Download \" buttons and download the Certificate file, Public key, and Private key file as shown in the following screenshot -","title":"5.3 Download Certificates:"},{"location":"AWS_IoT/#54-close-the-create-certificate-page","text":"Close the \"Create Certificate\" page by clicking on the \" Done \" button.","title":"5.4 Close the Create Certificate Page:"},{"location":"AWS_IoT/#54-download-amazon-root-ca-certificate","text":"Save the \" RSA 2048 bit key: Amazon Root CA 1 \" file from the following page as \" AmazonRootCA.crt \" - (Page URL - https://docs.aws.amazon.com/iot/latest/developerguide/managing-device-certs.html#server-authentication ) Note: Save all these Certificate and Key files on your machine as you would need to upload them on Raspberry Pi (Appendix-2) and on MQTT_to_DB Agent.","title":"5.4 Download Amazon Root CA Certificate:"},{"location":"AWS_IoT/#55-check-the-certificates-page","text":"Now on the \" Secure -- Certificates \" page you should see a new certificate. Note that your certificate is still inactive.","title":"5.5 Check the Certificates Page:"},{"location":"AWS_IoT/#6-activate-the-certificate","text":"Navigate to \" Secure -- Certificates \"; click on the Certificate options and select \" Activate \" to activate the certificate as shown in following screenshot -","title":"6. Activate the Certificate:"},{"location":"AWS_IoT/#7-associate-the-policies-with-certificate","text":"Navigate to \" Secure -- Certificates \" and click on the certificate to modify the certificate properties. On this certificate properties page click on \" Policies \" and from the \" Actions \" menu select \" Attach Policy \" as shown in the following screenshot - Select all the policies and click on \" Attach \" button as shown in the following screenshot - Your certificate Policies section should look like this -","title":"7. Associate the Policies with Certificate:"},{"location":"AWS_IoT/#8-locate-custom-endpoint","text":"You would need Custom endpoint to connect to the AWS IoT platform. It will be used by the MQTT clients as MQTT Broker host. You can locate your Customer Endpoint under \"Settings\" section as shown in the following screenshot -","title":"8. Locate Custom Endpoint:"},{"location":"LAB_access/","text":"Connectivity Check Lab access general description The lab has been built leveraging multiple cloud environments as following: Amazon Web Services Google Cloud Private intrastructure on-prem During this lab, you will get access to Google Cloud and Private Infrastructure, as these are providing container environment. Amazon Web Services is used in this setup as a message broker only which is a function already provided by Amazon. 1. On-prem private infrastructure access (VPN) In order to get access to private network, please first find credentials for your POD in the paper sheet on in the DMZ_USER_XX.txt file on the desktop. Each LAB User will have dedicate instance on Cisco Container Platform, where you can manage Kubernetes Cluster. Once you will create Kubernetes cluster, you will deploy your containerized application. Multiple Users are using same server, hence the naming POD0X-Y where X is a server number and Y is a environment ID within that server. Servers are running ESXi, and each ESXi is managed by own vCenter. Two to five users will share the same hardware server and same vCenter, however each User will have own instance of Cisco Container Platform. 1.1 Cisco Anyconnect Mobility Client Run Cisco Anyconnect VPN client available on your desktop. You\u2019ll need to review and configure the AnyConnect options. After Anyconnect launches, you\u2019ll need to click on the \u201cConfiguration\u201d button on the main panel. See image below. In case of the issues with certificate, you will need to uncheck the option that says \u201cBlock connections to untrusted servers\u201d. Your selection is immediately saved. Close the Configuration window. Enter VPN IP Address as provided in the paper sheet on your desk or on below screenshot. Then choose \u201cConnect\u201d. After a few seconds, you\u2019ll see a new window notifying you of an \u201cUntrusted Server Certificate\u201d. This is expected and not a real issue. Choose \u201cConnect Anyway\u201d. You\u2019ll see a new window prompting you to provide your Lab\u2019s network credentials. Enter the Username and Password as provided in the paper sheet. Choose \u201cOK\u201d. Next you\u2019ll see the main AnyConnect window go through several connection states. When it has completed establishing the connection, AnyConnect will iconify in the Notification Area of the Windows Taskbar. When you have an established VPN connection, the AnyConnect icon will display a symbol of a padlock. Your are connected to infrastructure on-prem. You can interact with resources in your lab by either using jumphost or accessing devices directly . 2 (Optional) Remote desktop connection This step is optional, you can perform all operations from the desktop as well. Open Remote Desktop Client (icon available on the desktop). If there is no icon for Remote Desktop Connection on the Desktop, click start, and type mstsc Click show options and type IP address of the jumphost and username. Don't forget to specify domain name which is HYBRIDLAB . Computer: 172.18.1.10 User name: HYBRIDLAB\\DMZ_USER_ ID Populated fields should be similar to the picture. You can find password in the paper sheet or in the DMZ_USER_XX.txt file. Each user will login to the same jumphost: 172.18.1.10 , regardless of the POD or server they should use 3. Accessing vCenter Users are grouped by PODs, each POD is managed by single VMWare vCenter instance. Please use respective vCenter for your POD according to below table: Table 1: User to vCenter anc CCP assignment VPN/AD User POD name vCenter URL CCP Control Plane URL DMZ_User_01 POD 01-A https://vc-pod01.hybridlab.local https://172.18.1.165 DMZ_User_02 POD 01-B https://vc-pod01.hybridlab.local https://172.18.1.169 DMZ_User_03 POD 02-A https://vc-pod02.hybridlab.local https://172.18.1.177 DMZ_User_04 POD 02-B https://vc-pod02.hybridlab.local https://172.18.1.181 DMZ_User_05 POD 03-A https://vc-pod03.hybridlab.local https://172.18.1.189 DMZ_User_06 POD 03-B https://vc-pod03.hybridlab.local https://172.18.1.193 DMZ_User_07 POD 04-A https://vc-pod04.hybridlab.local https://172.18.1.197 DMZ_User_08 POD 04-B https://vc-pod04.hybridlab.local https://172.18.1.201 DMZ_User_09 POD 04-C https://vc-pod04.hybridlab.local https://172.18.1.205 DMZ_User_10 POD 04-D https://vc-pod04.hybridlab.local https://172.18.1.209 DMZ_User_11 POD 06-A https://vc-pod06.hybridlab.local https://172.18.1.213 DMZ_User_12 POD 06-B https://vc-pod06.hybridlab.local https://172.18.1.217 DMZ_User_13 POD 06-C https://vc-pod06.hybridlab.local https://172.18.1.221 DMZ_User_14 POD 06-D https://vc-pod06.hybridlab.local https://172.18.1.225 DMZ_User_15 POD 06-E https://vc-pod06.hybridlab.local https://172.18.1.173 DMZ_User_16 POD 07-A https://vc-pod07.hybridlab.local https://172.18.1.229 DMZ_User_17 POD 07-B https://vc-pod07.hybridlab.local https://172.18.1.233 DMZ_User_18 POD 07-C https://vc-pod07.hybridlab.local https://172.18.1.237 DMZ_User_19 POD 07-D https://vc-pod07.hybridlab.local https://172.18.1.241 DMZ_User_20 POD 07-E https://vc-pod07.hybridlab.local https://172.18.1.185 To avoid issues with Flash Player, please select HTML5 mode for vCenter User Interface: Please enter your Active Directory credentials from the DMZ_USER_XX.txt file on your desktop, or paper sheet. You don't have to specify domain name. 4. Accessing Cisco Container Platform Cisco Container Platform manages Kuberenetes clusters in the private infrasturcture. You will have access to dedicated instance of Cisco Container Platform, from which you will manage you own Kuberenetes Clusters used later on to deploy application. Please refer to the Table 1 to access your own Cisco Container Platform dashboad. Use your Active Directory credentials to login without specifying domain name - see following picture: 5. Google Cloud access Open Chrome web browser from your desktop (you can use either jumphost or local PC) Go to http://cloud.google.com , click on sign-in in the top right corner Enter username for your lab pod which you can find in the paper sheet or DMZ_USER_XX.txt file on your desktop. You can change language to your preferred. Once logged in, click on the Console button in the top right corner to open Google Cloud Platform Console. When you sign-in to Google Cloud admin panel for the first time, you will be asked to accept Terms of Service, provide country of residence and email updates. Please select following options, and click AGREE AND CONTINUE Next, you will have to select project - click on the Select a project In the new window, select project `fwardz-001 Now you can access Google Kubernetes Engine - a Kubernetes Cluster in the Google Cloud You should see screen with warning about no sufficient rights to see the GKE Cluster object, however, you can still navigate to \"Workloads\" where you can deploy applications on the GKE Cluster.","title":"Accessing the Lab Environment"},{"location":"LAB_access/#connectivity-check","text":"","title":"Connectivity Check"},{"location":"LAB_access/#lab-access-general-description","text":"The lab has been built leveraging multiple cloud environments as following: Amazon Web Services Google Cloud Private intrastructure on-prem During this lab, you will get access to Google Cloud and Private Infrastructure, as these are providing container environment. Amazon Web Services is used in this setup as a message broker only which is a function already provided by Amazon.","title":"Lab access general description"},{"location":"LAB_access/#1-on-prem-private-infrastructure-access-vpn","text":"In order to get access to private network, please first find credentials for your POD in the paper sheet on in the DMZ_USER_XX.txt file on the desktop. Each LAB User will have dedicate instance on Cisco Container Platform, where you can manage Kubernetes Cluster. Once you will create Kubernetes cluster, you will deploy your containerized application. Multiple Users are using same server, hence the naming POD0X-Y where X is a server number and Y is a environment ID within that server. Servers are running ESXi, and each ESXi is managed by own vCenter. Two to five users will share the same hardware server and same vCenter, however each User will have own instance of Cisco Container Platform.","title":"1. On-prem private infrastructure access (VPN)"},{"location":"LAB_access/#11-cisco-anyconnect-mobility-client","text":"Run Cisco Anyconnect VPN client available on your desktop. You\u2019ll need to review and configure the AnyConnect options. After Anyconnect launches, you\u2019ll need to click on the \u201cConfiguration\u201d button on the main panel. See image below. In case of the issues with certificate, you will need to uncheck the option that says \u201cBlock connections to untrusted servers\u201d. Your selection is immediately saved. Close the Configuration window. Enter VPN IP Address as provided in the paper sheet on your desk or on below screenshot. Then choose \u201cConnect\u201d. After a few seconds, you\u2019ll see a new window notifying you of an \u201cUntrusted Server Certificate\u201d. This is expected and not a real issue. Choose \u201cConnect Anyway\u201d. You\u2019ll see a new window prompting you to provide your Lab\u2019s network credentials. Enter the Username and Password as provided in the paper sheet. Choose \u201cOK\u201d. Next you\u2019ll see the main AnyConnect window go through several connection states. When it has completed establishing the connection, AnyConnect will iconify in the Notification Area of the Windows Taskbar. When you have an established VPN connection, the AnyConnect icon will display a symbol of a padlock. Your are connected to infrastructure on-prem. You can interact with resources in your lab by either using jumphost or accessing devices directly .","title":"1.1 Cisco Anyconnect Mobility Client"},{"location":"LAB_access/#2-optional-remote-desktop-connection","text":"This step is optional, you can perform all operations from the desktop as well. Open Remote Desktop Client (icon available on the desktop). If there is no icon for Remote Desktop Connection on the Desktop, click start, and type mstsc Click show options and type IP address of the jumphost and username. Don't forget to specify domain name which is HYBRIDLAB . Computer: 172.18.1.10 User name: HYBRIDLAB\\DMZ_USER_ ID Populated fields should be similar to the picture. You can find password in the paper sheet or in the DMZ_USER_XX.txt file. Each user will login to the same jumphost: 172.18.1.10 , regardless of the POD or server they should use","title":"2 (Optional) Remote desktop connection"},{"location":"LAB_access/#3-accessing-vcenter","text":"Users are grouped by PODs, each POD is managed by single VMWare vCenter instance. Please use respective vCenter for your POD according to below table: Table 1: User to vCenter anc CCP assignment VPN/AD User POD name vCenter URL CCP Control Plane URL DMZ_User_01 POD 01-A https://vc-pod01.hybridlab.local https://172.18.1.165 DMZ_User_02 POD 01-B https://vc-pod01.hybridlab.local https://172.18.1.169 DMZ_User_03 POD 02-A https://vc-pod02.hybridlab.local https://172.18.1.177 DMZ_User_04 POD 02-B https://vc-pod02.hybridlab.local https://172.18.1.181 DMZ_User_05 POD 03-A https://vc-pod03.hybridlab.local https://172.18.1.189 DMZ_User_06 POD 03-B https://vc-pod03.hybridlab.local https://172.18.1.193 DMZ_User_07 POD 04-A https://vc-pod04.hybridlab.local https://172.18.1.197 DMZ_User_08 POD 04-B https://vc-pod04.hybridlab.local https://172.18.1.201 DMZ_User_09 POD 04-C https://vc-pod04.hybridlab.local https://172.18.1.205 DMZ_User_10 POD 04-D https://vc-pod04.hybridlab.local https://172.18.1.209 DMZ_User_11 POD 06-A https://vc-pod06.hybridlab.local https://172.18.1.213 DMZ_User_12 POD 06-B https://vc-pod06.hybridlab.local https://172.18.1.217 DMZ_User_13 POD 06-C https://vc-pod06.hybridlab.local https://172.18.1.221 DMZ_User_14 POD 06-D https://vc-pod06.hybridlab.local https://172.18.1.225 DMZ_User_15 POD 06-E https://vc-pod06.hybridlab.local https://172.18.1.173 DMZ_User_16 POD 07-A https://vc-pod07.hybridlab.local https://172.18.1.229 DMZ_User_17 POD 07-B https://vc-pod07.hybridlab.local https://172.18.1.233 DMZ_User_18 POD 07-C https://vc-pod07.hybridlab.local https://172.18.1.237 DMZ_User_19 POD 07-D https://vc-pod07.hybridlab.local https://172.18.1.241 DMZ_User_20 POD 07-E https://vc-pod07.hybridlab.local https://172.18.1.185 To avoid issues with Flash Player, please select HTML5 mode for vCenter User Interface: Please enter your Active Directory credentials from the DMZ_USER_XX.txt file on your desktop, or paper sheet. You don't have to specify domain name.","title":"3. Accessing vCenter"},{"location":"LAB_access/#4-accessing-cisco-container-platform","text":"Cisco Container Platform manages Kuberenetes clusters in the private infrasturcture. You will have access to dedicated instance of Cisco Container Platform, from which you will manage you own Kuberenetes Clusters used later on to deploy application. Please refer to the Table 1 to access your own Cisco Container Platform dashboad. Use your Active Directory credentials to login without specifying domain name - see following picture:","title":"4. Accessing Cisco Container Platform"},{"location":"LAB_access/#5-google-cloud-access","text":"Open Chrome web browser from your desktop (you can use either jumphost or local PC) Go to http://cloud.google.com , click on sign-in in the top right corner Enter username for your lab pod which you can find in the paper sheet or DMZ_USER_XX.txt file on your desktop. You can change language to your preferred. Once logged in, click on the Console button in the top right corner to open Google Cloud Platform Console. When you sign-in to Google Cloud admin panel for the first time, you will be asked to accept Terms of Service, provide country of residence and email updates. Please select following options, and click AGREE AND CONTINUE Next, you will have to select project - click on the Select a project In the new window, select project `fwardz-001 Now you can access Google Kubernetes Engine - a Kubernetes Cluster in the Google Cloud You should see screen with warning about no sufficient rights to see the GKE Cluster object, however, you can still navigate to \"Workloads\" where you can deploy applications on the GKE Cluster.","title":"5. Google Cloud access"},{"location":"RaPi_with_Sensor/","text":"Appendix - 2: Raspberry Pi with DHT22 Sensor and Docker Container 1. Hardware and Software Requirements: Following are the Hardware and Software prerequisites that you need before you begin with this article - 1.1 Hardware: Raspberry Pi with Power Adapter SD Card and SD Card Reader Ethernet Cable DHT22 Sensor 10K Ohm Resistor Jumper wires and Breadboard Keyboard and Monitor (Optional) 1.2 Software: Official Raspbian Image Etcher for flashing Raspbian OS image into theSD card 2. DHT22 Sensor Connection with Raspberry Pi You can connect the DHT22 sensor with Raspberry Pi as shown in the following image. Pin 1 and Pin 4 on the sensor should be connected to the 3.3V Power Pin and Ground Pin respectively. Pin 2 on the sensor should be connected to GPIO Pin 4. 3. Burn Raspbian OS on the SD Card and enable SSH Download the Raspbian OS and use [Etcher] (https://www.balena.io/etcher/) to install it on the SD card. For headless setup, open the SD Card on your computer, after the installation process gets completed to enable SSH. SSH can be enabled by placing a file named ssh, without any extension, onto the boot partition of the SD card from another computer. When the Pi boots, it looks for the ssh file. If it is found, SSH is enabled and the file is deleted. The content of the file does not matter; it could contain text, or nothing at all. If you have loaded Raspbian onto a blank SD card, you will have two partitions. The first one, which is the smaller one, is the boot partition. Place the file into this one. Now plug the SD card into your Raspberry Pi and power it on. 4. SSH into the Raspberry Pi and Install Docker Runtime SSH into your Raspberry Pi using the following command (replace with the IP Address assigned to your raspberry Pi) - ssh pi@ ip address On the password prompt you could use the default raspberry password - \" raspberry \" Now you should have access to the shell. Upgrade raspbian packages using the following command - sudo apt-get update sudo apt-get upgrade Install Docker runtime using the following command - curl -sSL https://get.docker.com | sh Add permission to Pi user to run Docker commands by adding \u201cpi\u201d user to \u201cdocker\u201d group using the following command \u2013 sudo usermod -aG docker pi You must Log off from Raspberry Pi and Login again, for this to take effect. Check Docker installation using the following command - docker --version If you see the correct version, you are good to go. 5. Download Required Files and Upload on Raspberry Pi: 5.1 Download the Certificate files from from AWS IoT core as described in Appendix - 1 5.2 Download the \" settings.ini \" from github repo - Link 5.3 Update the settings file variables accordingly. 5.4 Login to Raspberry Pi and create new settings folder under the 'pi' user's home directory, using the following command - mkdir /home/pi/settings 5.5 Upload the AWS Certificate Files and updated \"settings.ini\" file on Raspberry Pi using sftp - sftp pi@ ip-address cd /home/pi/settings put file-name Note: Make sure you copy the updated settings.ini, Amazon RootCA Certificate file, AWS IoT Core Things Certificate Private Key files into the '/home/pi/settings' directory on your Raspberry Pi. 5.6 Close the sftp session using the following command - bye 6. Test the Container on Raspberry Pi: Login back into the Raspberry Pi and run the sensor container in an interactive mode using the following command - sudo docker run --privileged -it -v /home/pi/settings:/usr/src/app/settings pradeesi/rapi_cl_hc_sensor 7. Configure Raspberry Pi to Trigger the Sensor Container at Startup: 7.1 open the 'rc.local' using the following command - sudo vi /etc/rc.local 7.2 Add following command at the top of this file (preferably in the first line) - sudo docker run --privileged -d -v /home/pi/settings:/usr/src/app/settings pradeesi/rapi_cl_hc_sensor 7.3 Save the file using ' Esc + :wq ' command on vi editor. 7.4 Change the execution bit using the following command - sudo chmod +x /etc/rc.local 7.5 Reboot the Raspberry Pi using ' sudo reboot '. 7.6 After the reboot process gets completed, login back to the Raspberry Pi and check if the sensor container is running using the command ' sudo docker ps '. Note: If the docker container is not getting triggered on system startup, add 5 second delay by adding ' sleep 5 ' before the docker command added in the 'rc.local' file (in Step# 7.2).","title":"Appendix-2 Sensor Setup on Raspberry Pi"},{"location":"RaPi_with_Sensor/#appendix-2-raspberry-pi-with-dht22-sensor-and-docker-container","text":"","title":"Appendix - 2: Raspberry Pi with DHT22 Sensor and Docker Container"},{"location":"RaPi_with_Sensor/#1-hardware-and-software-requirements","text":"Following are the Hardware and Software prerequisites that you need before you begin with this article -","title":"1. Hardware and Software Requirements:"},{"location":"RaPi_with_Sensor/#11-hardware","text":"Raspberry Pi with Power Adapter SD Card and SD Card Reader Ethernet Cable DHT22 Sensor 10K Ohm Resistor Jumper wires and Breadboard Keyboard and Monitor (Optional)","title":"1.1 Hardware:"},{"location":"RaPi_with_Sensor/#12-software","text":"Official Raspbian Image Etcher for flashing Raspbian OS image into theSD card","title":"1.2 Software:"},{"location":"RaPi_with_Sensor/#2-dht22-sensor-connection-with-raspberry-pi","text":"You can connect the DHT22 sensor with Raspberry Pi as shown in the following image. Pin 1 and Pin 4 on the sensor should be connected to the 3.3V Power Pin and Ground Pin respectively. Pin 2 on the sensor should be connected to GPIO Pin 4.","title":"2. DHT22 Sensor Connection with Raspberry Pi"},{"location":"RaPi_with_Sensor/#3-burn-raspbian-os-on-the-sd-card-and-enable-ssh","text":"Download the Raspbian OS and use [Etcher] (https://www.balena.io/etcher/) to install it on the SD card. For headless setup, open the SD Card on your computer, after the installation process gets completed to enable SSH. SSH can be enabled by placing a file named ssh, without any extension, onto the boot partition of the SD card from another computer. When the Pi boots, it looks for the ssh file. If it is found, SSH is enabled and the file is deleted. The content of the file does not matter; it could contain text, or nothing at all. If you have loaded Raspbian onto a blank SD card, you will have two partitions. The first one, which is the smaller one, is the boot partition. Place the file into this one. Now plug the SD card into your Raspberry Pi and power it on.","title":"3. Burn Raspbian OS on the SD Card and enable SSH"},{"location":"RaPi_with_Sensor/#4-ssh-into-the-raspberry-pi-and-install-docker-runtime","text":"SSH into your Raspberry Pi using the following command (replace with the IP Address assigned to your raspberry Pi) - ssh pi@ ip address On the password prompt you could use the default raspberry password - \" raspberry \" Now you should have access to the shell. Upgrade raspbian packages using the following command - sudo apt-get update sudo apt-get upgrade Install Docker runtime using the following command - curl -sSL https://get.docker.com | sh Add permission to Pi user to run Docker commands by adding \u201cpi\u201d user to \u201cdocker\u201d group using the following command \u2013 sudo usermod -aG docker pi You must Log off from Raspberry Pi and Login again, for this to take effect. Check Docker installation using the following command - docker --version If you see the correct version, you are good to go.","title":"4. SSH into the Raspberry Pi and Install Docker Runtime"},{"location":"RaPi_with_Sensor/#5-download-required-files-and-upload-on-raspberry-pi","text":"5.1 Download the Certificate files from from AWS IoT core as described in Appendix - 1 5.2 Download the \" settings.ini \" from github repo - Link 5.3 Update the settings file variables accordingly. 5.4 Login to Raspberry Pi and create new settings folder under the 'pi' user's home directory, using the following command - mkdir /home/pi/settings 5.5 Upload the AWS Certificate Files and updated \"settings.ini\" file on Raspberry Pi using sftp - sftp pi@ ip-address cd /home/pi/settings put file-name Note: Make sure you copy the updated settings.ini, Amazon RootCA Certificate file, AWS IoT Core Things Certificate Private Key files into the '/home/pi/settings' directory on your Raspberry Pi. 5.6 Close the sftp session using the following command - bye","title":"5. Download Required Files and Upload on Raspberry Pi:"},{"location":"RaPi_with_Sensor/#6-test-the-container-on-raspberry-pi","text":"Login back into the Raspberry Pi and run the sensor container in an interactive mode using the following command - sudo docker run --privileged -it -v /home/pi/settings:/usr/src/app/settings pradeesi/rapi_cl_hc_sensor","title":"6. Test the Container on Raspberry Pi:"},{"location":"RaPi_with_Sensor/#7-configure-raspberry-pi-to-trigger-the-sensor-container-at-startup","text":"7.1 open the 'rc.local' using the following command - sudo vi /etc/rc.local 7.2 Add following command at the top of this file (preferably in the first line) - sudo docker run --privileged -d -v /home/pi/settings:/usr/src/app/settings pradeesi/rapi_cl_hc_sensor 7.3 Save the file using ' Esc + :wq ' command on vi editor. 7.4 Change the execution bit using the following command - sudo chmod +x /etc/rc.local 7.5 Reboot the Raspberry Pi using ' sudo reboot '. 7.6 After the reboot process gets completed, login back to the Raspberry Pi and check if the sensor container is running using the command ' sudo docker ps '. Note: If the docker container is not getting triggered on system startup, add 5 second delay by adding ' sleep 5 ' before the docker command added in the 'rc.local' file (in Step# 7.2).","title":"7. Configure Raspberry Pi to Trigger the Sensor Container at Startup:"},{"location":"backend_exercise/","text":"Explore Backend App and Kubernetes Dashboard: Your Hybrid Cloud App's backend compoents (MariaDB, MQTT DB Agent, and REST API agent) should be up and running. Now let's explore some low level details to understand the application containers and kubernetes better. Task 1: Find the size of the 'Persistent Volume Claim' used for MariaDB database? Execute ' kubectl get pvc mariadb-pv-claim ' on kubernetes master node and check 'CAPACITY' value. Task 2: Check if the 'MQTT to DB Agent' is receiving data from AWS IoT. (Hint: check the logs in 'MQTT to DB Agent' Pod) Find the pod name for 'MQTT to DB Agent' using the kubectl command ' kubectl get pods ' and locate the pod name starting with 'iot-backend-mqtt-db-agent-' Check the container logs using the kubectl command ' kubectl logs \\ pod name> ' (replace the pod name with correct value). You should see the Json messages received from AWS IoT platform. Task 3: Assuming that the 'MQTT to DB Agent' connects to the AWS IoT Core on port '8883' and to MariaDB on port '3306'; connect with the 'iot-backend-mqtt-db-agent' pod and check the active connections. Find the pod name for 'MQTT to DB Agent' using the kubectl command ' kubectl get pods ' and locate the pod name starting with 'iot-backend-mqtt-db-agent-'. login to the container using the kubectl command ' kubectl exec -it \\ pod name> /bin/ash ' (replace the pod name with correct value). On the container shell execute the command ' netstat -n ' and check the 'Foreign Address' column. Look for port number '8883' and '3306'. Exit the container shell using ' exit ' command. Task 4: Login to the MariaDB database and explore the data tables. Find the pod name for 'MariaDB' using the kubectl command ' kubectl get pods ' and locate the pod name starting with 'iot-backend-mariadb-'. login to the MariaDB container using the kubectl command ' kubectl exec -it \\ pod name> /bin/bash ' (replace the pod name with correct value). Execute ' ss -tulpn ' and check the port MariaDB is listing to for incoming connections. You can also check the version of MariaDB using the shell command ' mysql --version ' Now connect to the MariaDB from the container shell and check Databases and Tables. Use ' mysql -u root -pcisco123 ' command to login to MaraDB. On mariaDB shell, use ' show databases; ' command to list all the databases. Look for ' sensor_db ' in the output (this is the database where we are storing the sensor data). Switch to sensor_db using the command ' use sensor_db; ' and list all the tables in this database using the command ' show tables; '. You should see only one table with the name 'sensor_data'; Try to list the data from this table using the SQL statement 'select * from sensor_data; '. Now check the record count in this table using the ' select count(*) from sensor_data; ' sql statement. Repeat the SQL from step 8 several times and check if the record count is increasing (each sensor would send the data after every 10 seconds). Use ' exit ' command at MariaDB prompt to exit the DB shell. Use ' exit ' command again to exit 'iot-backend-mariadb' container shell. Task 5: Connect to the REST API Agent container and find the port it is listing on for incoming REST calls. Find the pod name for 'REST API Agent' using the kubectl command ' kubectl get pods ' and locate the pod name starting with 'iot-backend-rest-api-agent-' (you may see more than one pod for this service, pick any one). login to the REST API Agent container using the kubectl command ' kubectl exec -it \\ pod name> /bin/ash ' (replace the pod name with correct value). Execute ' netstat -an ' command on the container shell and check the output. This container listens on port '5050' for incoming REST connections and connects with MariaDB using port '3306' (Connection to DB will timeout in case there are no requests coming in). Use ' exit ' command to come out of the container shell. Task 6: Check the logs messages from 'REST API Agent'. Find the pod name for 'REST API Agent' using the kubectl command ' kubectl get pods ' and locate the pod name starting with 'iot-backend-rest-api-agent-' (you may see more than one pod for this service, pick any one). check the logs using the kubectl command ' kubectl logs \\ pod name> ' Task 7: How the traffic would be distributed, if you have multiple Kubernetes Pods behind a Kubernetes NodePort Service? Right click on top of your putty window and click on \"Duplicate Session\" (You should have two putty windows side by side logged into same kubernetes master node). In the first putty window (logged into kubernetes master) execute the command ' kubectl get nodes -o ' and note down the Kubernetes master node's external IP address. Open a web browser on your machine and access the following url (add the master node ip in the url) - http:// kubernetes node's external ip :30500/temperature In the first putty window, run ' kubectl get pods ' on the Kubernetes Master. In the output you should see 2 pods for 'iot-backend-rest-api-agent'. In the first putty window, run the command ' watch kubectl logs \\ first pod name> ' (Use the first 'iot-backend-rest-api-agent' pod out of 2 pods that you saw in the output of the command executed in step#5). In the Second putty window, display the logs for second 'iot-backend-rest-api-agent' pod using the command ' watch kubectl logs \\ second pod name> '. Refresh or reload the web page with the url you used in step 4. Repeat step 8 and check the requests hitting the kubernetes pods in the log messages. Task 8: Login to Kubernetes Dashboard and explore the Backend App components. Kubernetes Dashboard > Nodes : You would see the Kubernetes Cluster Nodes on this screen. Kubernetes Dashboard > Persistent volume : You should see all the persistent values. Try to locate the persistent volume created for MariaDB. Kubernetes Dashboard > Overview > Workloads > deployments : You should see all the app components with the number of pods for each deployment. Kubernetes Dashboard > Overview > Workloads > pods : On this screen you should see all the pods for your application. Kubernetes Dashboard > Overview > Workloads > Replica Set : Click on a deployment name to view the detaild of Pods and Containers. Kubernetes Dashboard > Overview > Workloads > Replica Set : Click on a Logs icon on right hand side to view the container logs. Kubernetes Dashboard > Discovery and Load Balancing > Services : On this screen you would see list of Kubernetes Services deployed on your cluster. Kubernetes Dashboard > Config and Storage > Secrets : This screen would show the secret created for MariaDB.","title":"Explore Backend Application"},{"location":"backend_exercise/#explore-backend-app-and-kubernetes-dashboard","text":"Your Hybrid Cloud App's backend compoents (MariaDB, MQTT DB Agent, and REST API agent) should be up and running. Now let's explore some low level details to understand the application containers and kubernetes better. Task 1: Find the size of the 'Persistent Volume Claim' used for MariaDB database? Execute ' kubectl get pvc mariadb-pv-claim ' on kubernetes master node and check 'CAPACITY' value. Task 2: Check if the 'MQTT to DB Agent' is receiving data from AWS IoT. (Hint: check the logs in 'MQTT to DB Agent' Pod) Find the pod name for 'MQTT to DB Agent' using the kubectl command ' kubectl get pods ' and locate the pod name starting with 'iot-backend-mqtt-db-agent-' Check the container logs using the kubectl command ' kubectl logs \\ pod name> ' (replace the pod name with correct value). You should see the Json messages received from AWS IoT platform. Task 3: Assuming that the 'MQTT to DB Agent' connects to the AWS IoT Core on port '8883' and to MariaDB on port '3306'; connect with the 'iot-backend-mqtt-db-agent' pod and check the active connections. Find the pod name for 'MQTT to DB Agent' using the kubectl command ' kubectl get pods ' and locate the pod name starting with 'iot-backend-mqtt-db-agent-'. login to the container using the kubectl command ' kubectl exec -it \\ pod name> /bin/ash ' (replace the pod name with correct value). On the container shell execute the command ' netstat -n ' and check the 'Foreign Address' column. Look for port number '8883' and '3306'. Exit the container shell using ' exit ' command. Task 4: Login to the MariaDB database and explore the data tables. Find the pod name for 'MariaDB' using the kubectl command ' kubectl get pods ' and locate the pod name starting with 'iot-backend-mariadb-'. login to the MariaDB container using the kubectl command ' kubectl exec -it \\ pod name> /bin/bash ' (replace the pod name with correct value). Execute ' ss -tulpn ' and check the port MariaDB is listing to for incoming connections. You can also check the version of MariaDB using the shell command ' mysql --version ' Now connect to the MariaDB from the container shell and check Databases and Tables. Use ' mysql -u root -pcisco123 ' command to login to MaraDB. On mariaDB shell, use ' show databases; ' command to list all the databases. Look for ' sensor_db ' in the output (this is the database where we are storing the sensor data). Switch to sensor_db using the command ' use sensor_db; ' and list all the tables in this database using the command ' show tables; '. You should see only one table with the name 'sensor_data'; Try to list the data from this table using the SQL statement 'select * from sensor_data; '. Now check the record count in this table using the ' select count(*) from sensor_data; ' sql statement. Repeat the SQL from step 8 several times and check if the record count is increasing (each sensor would send the data after every 10 seconds). Use ' exit ' command at MariaDB prompt to exit the DB shell. Use ' exit ' command again to exit 'iot-backend-mariadb' container shell. Task 5: Connect to the REST API Agent container and find the port it is listing on for incoming REST calls. Find the pod name for 'REST API Agent' using the kubectl command ' kubectl get pods ' and locate the pod name starting with 'iot-backend-rest-api-agent-' (you may see more than one pod for this service, pick any one). login to the REST API Agent container using the kubectl command ' kubectl exec -it \\ pod name> /bin/ash ' (replace the pod name with correct value). Execute ' netstat -an ' command on the container shell and check the output. This container listens on port '5050' for incoming REST connections and connects with MariaDB using port '3306' (Connection to DB will timeout in case there are no requests coming in). Use ' exit ' command to come out of the container shell. Task 6: Check the logs messages from 'REST API Agent'. Find the pod name for 'REST API Agent' using the kubectl command ' kubectl get pods ' and locate the pod name starting with 'iot-backend-rest-api-agent-' (you may see more than one pod for this service, pick any one). check the logs using the kubectl command ' kubectl logs \\ pod name> ' Task 7: How the traffic would be distributed, if you have multiple Kubernetes Pods behind a Kubernetes NodePort Service? Right click on top of your putty window and click on \"Duplicate Session\" (You should have two putty windows side by side logged into same kubernetes master node). In the first putty window (logged into kubernetes master) execute the command ' kubectl get nodes -o ' and note down the Kubernetes master node's external IP address. Open a web browser on your machine and access the following url (add the master node ip in the url) - http:// kubernetes node's external ip :30500/temperature In the first putty window, run ' kubectl get pods ' on the Kubernetes Master. In the output you should see 2 pods for 'iot-backend-rest-api-agent'. In the first putty window, run the command ' watch kubectl logs \\ first pod name> ' (Use the first 'iot-backend-rest-api-agent' pod out of 2 pods that you saw in the output of the command executed in step#5). In the Second putty window, display the logs for second 'iot-backend-rest-api-agent' pod using the command ' watch kubectl logs \\ second pod name> '. Refresh or reload the web page with the url you used in step 4. Repeat step 8 and check the requests hitting the kubernetes pods in the log messages. Task 8: Login to Kubernetes Dashboard and explore the Backend App components. Kubernetes Dashboard > Nodes : You would see the Kubernetes Cluster Nodes on this screen. Kubernetes Dashboard > Persistent volume : You should see all the persistent values. Try to locate the persistent volume created for MariaDB. Kubernetes Dashboard > Overview > Workloads > deployments : You should see all the app components with the number of pods for each deployment. Kubernetes Dashboard > Overview > Workloads > pods : On this screen you should see all the pods for your application. Kubernetes Dashboard > Overview > Workloads > Replica Set : Click on a deployment name to view the detaild of Pods and Containers. Kubernetes Dashboard > Overview > Workloads > Replica Set : Click on a Logs icon on right hand side to view the container logs. Kubernetes Dashboard > Discovery and Load Balancing > Services : On this screen you would see list of Kubernetes Services deployed on your cluster. Kubernetes Dashboard > Config and Storage > Secrets : This screen would show the secret created for MariaDB.","title":"Explore Backend App and Kubernetes Dashboard:"},{"location":"basic_kubectl_cmds/","text":"kubectl version kubectl get nodes kubectl run --image= --port= Kubernetes Deployments: A Deployment controller provides declarative updates for Pods and ReplicaSets. You describe a desired state in a Deployment object, and the Deployment controller changes the actual state to the desired state at a controlled rate. You can define Deployments to create new ReplicaSets, or to remove existing Deployments and adopt all their resources with new Deployments. Create Deployment Use the following command to create a deployment using a yaml file - kubectl create -f yaml file path List Deployments: Use the following command to list all kubernetes deployments - kubectl get deployments Deployment Details: Use the following command to display the details of deployment - kubectl describe deployment Scale Deployment: Use the following command to scale (up/down) a kubernetes deployment - kubectl scale deployments/ deployment_name --replicas= number of replicas Delete Deployment: Use the following command to delete a deployment - kubectl delete deployment deployment_name Kubernetes Pods: A Pod represents a unit of deployment: a single instance of an application in Kubernetes, which might consist of either a single container or a small number of containers that are tightly coupled and that share resources. List Pods: Use the following command to list all the pods: kubectl get pods or use wide option to see more details - kubectl get pods -o wide List Pods Filter: You can filter the pods using the labels used in deployment - kubectl get pods -l label_name = label_value Pod Details: Use the following command to see the containers and used images for pods - kubectl describe pods Pod Logs: Use the following command to check the pod logs - kubectl logs pod_name Kubernetes Services: A Kubernetes Service is an abstraction which defines a logical set of Pods and a policy by which to access them - sometimes called a micro-service. List Service: Use the following command to list the current Services - kubectl get services You can filter the services using the labels used in deployment - kubectl get services -l label_name = label_value Create Service: Use the following command to create a new service - kubectl expose deployment/ deployment_name --type=\"NodePort\" --port port Service Details: Use the following command to find out what port was opened externally (by the NodePort option) - kubectl describe services/ service_name Delete Service: Use the following command to delete a service - kubectl delete service/ service_name or kubectl delete service -l label_name = label_value Parse Service Node Port: Use the following script to filterout the node-port of a service (change the service name) - export NODE_PORT=$(kubectl get services/ service-name -o go-template='{{(index .spec.ports 0).nodePort}}') echo NODE_PORT=$NODE_PORT Kubernetes Secrets: A Secret is an object that stores a piece of sensitive data like a password or key. List Secrets: Use the following command to list all secrets kubectl get secrets Secret Details: Use the following command to list the secret details - kubectl describe secrets/ secret_name Create Secret: Use the following command to create secret - kubectl create secret generic secret_name --from-literal= key_name = key_value Delete Secret: use the following command to delete a secret - kubectl delete secret secret_name Interacting with Pod Containers List Env Variables: Use the following command to list the environment variables - kubectl exec pod_name env Access Container Shell: Use the following command to access bash shell in a container - kubectl exec -ti pod_name bash Note: To close your container connection type ' exit '. ========================================= kubectl proxy curl http://localhost:8001/version export POD_NAME=$(kubectl get pods -o go-template --template '{{range .items}}{{.metadata.name}}{{\"\\n\"}}{{end}}') echo Name of the Pod: $POD_NAME curl http://localhost:8001/api/v1/namespaces/default/pods/$POD_NAME/proxy =========================================","title":"Basic kubectl cmds"},{"location":"basic_kubectl_cmds/#kubernetes-deployments","text":"A Deployment controller provides declarative updates for Pods and ReplicaSets. You describe a desired state in a Deployment object, and the Deployment controller changes the actual state to the desired state at a controlled rate. You can define Deployments to create new ReplicaSets, or to remove existing Deployments and adopt all their resources with new Deployments. Create Deployment Use the following command to create a deployment using a yaml file - kubectl create -f yaml file path List Deployments: Use the following command to list all kubernetes deployments - kubectl get deployments Deployment Details: Use the following command to display the details of deployment - kubectl describe deployment Scale Deployment: Use the following command to scale (up/down) a kubernetes deployment - kubectl scale deployments/ deployment_name --replicas= number of replicas Delete Deployment: Use the following command to delete a deployment - kubectl delete deployment deployment_name","title":"Kubernetes Deployments:"},{"location":"basic_kubectl_cmds/#kubernetes-pods","text":"A Pod represents a unit of deployment: a single instance of an application in Kubernetes, which might consist of either a single container or a small number of containers that are tightly coupled and that share resources. List Pods: Use the following command to list all the pods: kubectl get pods or use wide option to see more details - kubectl get pods -o wide List Pods Filter: You can filter the pods using the labels used in deployment - kubectl get pods -l label_name = label_value Pod Details: Use the following command to see the containers and used images for pods - kubectl describe pods Pod Logs: Use the following command to check the pod logs - kubectl logs pod_name","title":"Kubernetes Pods:"},{"location":"basic_kubectl_cmds/#kubernetes-services","text":"A Kubernetes Service is an abstraction which defines a logical set of Pods and a policy by which to access them - sometimes called a micro-service. List Service: Use the following command to list the current Services - kubectl get services You can filter the services using the labels used in deployment - kubectl get services -l label_name = label_value Create Service: Use the following command to create a new service - kubectl expose deployment/ deployment_name --type=\"NodePort\" --port port Service Details: Use the following command to find out what port was opened externally (by the NodePort option) - kubectl describe services/ service_name Delete Service: Use the following command to delete a service - kubectl delete service/ service_name or kubectl delete service -l label_name = label_value Parse Service Node Port: Use the following script to filterout the node-port of a service (change the service name) - export NODE_PORT=$(kubectl get services/ service-name -o go-template='{{(index .spec.ports 0).nodePort}}') echo NODE_PORT=$NODE_PORT","title":"Kubernetes Services:"},{"location":"basic_kubectl_cmds/#kubernetes-secrets","text":"A Secret is an object that stores a piece of sensitive data like a password or key. List Secrets: Use the following command to list all secrets kubectl get secrets Secret Details: Use the following command to list the secret details - kubectl describe secrets/ secret_name Create Secret: Use the following command to create secret - kubectl create secret generic secret_name --from-literal= key_name = key_value Delete Secret: use the following command to delete a secret - kubectl delete secret secret_name","title":"Kubernetes Secrets:"},{"location":"basic_kubectl_cmds/#interacting-with-pod-containers","text":"List Env Variables: Use the following command to list the environment variables - kubectl exec pod_name env Access Container Shell: Use the following command to access bash shell in a container - kubectl exec -ti pod_name bash Note: To close your container connection type ' exit '.","title":"Interacting with Pod Containers"},{"location":"basic_kubectl_cmds/#_1","text":"kubectl proxy curl http://localhost:8001/version export POD_NAME=$(kubectl get pods -o go-template --template '{{range .items}}{{.metadata.name}}{{\"\\n\"}}{{end}}') echo Name of the Pod: $POD_NAME curl http://localhost:8001/api/v1/namespaces/default/pods/$POD_NAME/proxy =========================================","title":"========================================="},{"location":"ccp_accessing_kubernetes_cluster/","text":"Accessing Kubernetes Cluster Once cluster installation has finished, you will see the status READY in CCP dashboard. Accessing new Kubernetes cluster requires to obtain the kubeconfig file. Click on the name of your cluster to see the detailed view. From the same context menu you can enter to detailed more of the cluster, access Kubernetes dashboard or monitoring based on Prometheus and Kibana or even a delete cluster. Next task is to enter into the detailed view of the cluster, here you can also download kubeconfig file. Next, click on the Kubernetes Dashboard button: You will be redirected to Kubernetes standard dashboard. Cisco did not changed the Kubernetes User Interface intentionaly, since it is well known by developers. Please select Kubeconfig option (which is the default) and select previously stored kubeconfig.yaml file which should be in the Downloads folder. Once selected, click Sing-In: After successful login you will be redirected to the default namespace view: Kubectl - Kubernetes Command Line Interface While most of the options in Kubernetes are available through the dashboard, CLI commands are also available, and convenient to use. Kubectl is a software leveraging Kubernetes API and translate commands to specific API calls. In the later excercises you will you kubectl commands to deploy application. You can login via SSH to your Master node using PuTTY application available on your desktop on the jumphost. Step 1 Obtain IP address of your Kubernetes Master node. Login to Cisco Container Platform, navigate to the following path: CCP Dashboard - Clusters - click on the View Details from the drop down option of your Cluster - Look at the section default-master-pool - note the Public IP Address Step 2 SSH to Master Node using PuTTY Login to Kubernetes node require SSH private key. The Key is located on the Jumphost in the folder C:\\ssh-key\\id_rsa.pem . Open PuTTY, go to Connection - SSH - Auth and select private key - next go to Connection - Data and provide username ccpuser - Once logged in to Kubernetes master node, you can use kubectl command. You can try example command to obtain nodes information. kubectl get nodes -o wide","title":"Access Kubernetes Cluster (Tenant Cluster)"},{"location":"ccp_accessing_kubernetes_cluster/#accessing-kubernetes-cluster","text":"Once cluster installation has finished, you will see the status READY in CCP dashboard. Accessing new Kubernetes cluster requires to obtain the kubeconfig file. Click on the name of your cluster to see the detailed view. From the same context menu you can enter to detailed more of the cluster, access Kubernetes dashboard or monitoring based on Prometheus and Kibana or even a delete cluster. Next task is to enter into the detailed view of the cluster, here you can also download kubeconfig file. Next, click on the Kubernetes Dashboard button: You will be redirected to Kubernetes standard dashboard. Cisco did not changed the Kubernetes User Interface intentionaly, since it is well known by developers. Please select Kubeconfig option (which is the default) and select previously stored kubeconfig.yaml file which should be in the Downloads folder. Once selected, click Sing-In: After successful login you will be redirected to the default namespace view:","title":"Accessing Kubernetes Cluster"},{"location":"ccp_accessing_kubernetes_cluster/#kubectl-kubernetes-command-line-interface","text":"While most of the options in Kubernetes are available through the dashboard, CLI commands are also available, and convenient to use. Kubectl is a software leveraging Kubernetes API and translate commands to specific API calls. In the later excercises you will you kubectl commands to deploy application. You can login via SSH to your Master node using PuTTY application available on your desktop on the jumphost. Step 1 Obtain IP address of your Kubernetes Master node. Login to Cisco Container Platform, navigate to the following path: CCP Dashboard - Clusters - click on the View Details from the drop down option of your Cluster - Look at the section default-master-pool - note the Public IP Address Step 2 SSH to Master Node using PuTTY Login to Kubernetes node require SSH private key. The Key is located on the Jumphost in the folder C:\\ssh-key\\id_rsa.pem . Open PuTTY, go to Connection - SSH - Auth and select private key - next go to Connection - Data and provide username ccpuser - Once logged in to Kubernetes master node, you can use kubectl command. You can try example command to obtain nodes information. kubectl get nodes -o wide","title":"Kubectl - Kubernetes Command Line Interface"},{"location":"ccp_create_cluster/","text":"Create Tenant Data Cluster After login to Cisco Container Platform, click New Cluster button, you will be redirected to the new page where you would have to provide details of your new Kubernetes cluster. During creation, you will be asked to specify some parameters. Please open google sheet and find your POD ID in the tab. Google Sheet Step 1 - Basic Information - select infrastructure provider, Kubernetes cluster name and Container Network Interface: In this step you have selected \"Calico\" as a Container Networking Interface, however there are other two supported - Cisco ACI and Contiv-VPP. Cisco ACI integration is done automatically during new Kubernetes cluster creation, CCP configures tenant in ACI with required network policies and Policy Based Routing that is used as a Load-Balancing services in hardware. Step 2 - Provider Settings - infrastructure provider details such as storage, vSwitch port-group and base image with approriate Kubernetes version: This lab is based on regular UCS servers, however, CCP in combination with Cisco hyperconverged infrastructure - Hyperflex results in best in class performance. CCP can automatically provision dynamic persistent volumes directly on Hyperflex, rather on vmware, bypassing extra layer of virtualisation. Step 3 - Node configuration - here you can configure node sizing, provide access information such as public SSH key, Load-Balancer VIP, subnet for PODs. Do not enable service-mesh based on Istio 1.0 and finally provide rigths for managing kubernetes clusters in AWS public cloud. On the next screen you will see option to enable Harbor Registry , to enable local docker image. At this point please do not select this option. Click Next to enter the summary page, and just confirm all data are valid according to google sheet. Once confirmed please click Finish . Once finished, you will see progress bar to check status of the cluster creation. Monitor cluster creation You can observe tenant cluster creation from CCP Dashboard, however, if you are interested to see more details, you can login to vCenter and monitor VM cloning process.","title":"Create Kubernetes Cluster on CCP (Tenant Cluster)"},{"location":"ccp_create_cluster/#create-tenant-data-cluster","text":"After login to Cisco Container Platform, click New Cluster button, you will be redirected to the new page where you would have to provide details of your new Kubernetes cluster. During creation, you will be asked to specify some parameters. Please open google sheet and find your POD ID in the tab. Google Sheet Step 1 - Basic Information - select infrastructure provider, Kubernetes cluster name and Container Network Interface: In this step you have selected \"Calico\" as a Container Networking Interface, however there are other two supported - Cisco ACI and Contiv-VPP. Cisco ACI integration is done automatically during new Kubernetes cluster creation, CCP configures tenant in ACI with required network policies and Policy Based Routing that is used as a Load-Balancing services in hardware. Step 2 - Provider Settings - infrastructure provider details such as storage, vSwitch port-group and base image with approriate Kubernetes version: This lab is based on regular UCS servers, however, CCP in combination with Cisco hyperconverged infrastructure - Hyperflex results in best in class performance. CCP can automatically provision dynamic persistent volumes directly on Hyperflex, rather on vmware, bypassing extra layer of virtualisation. Step 3 - Node configuration - here you can configure node sizing, provide access information such as public SSH key, Load-Balancer VIP, subnet for PODs. Do not enable service-mesh based on Istio 1.0 and finally provide rigths for managing kubernetes clusters in AWS public cloud. On the next screen you will see option to enable Harbor Registry , to enable local docker image. At this point please do not select this option. Click Next to enter the summary page, and just confirm all data are valid according to google sheet. Once confirmed please click Finish . Once finished, you will see progress bar to check status of the cluster creation.","title":"Create Tenant Data Cluster"},{"location":"ccp_create_cluster/#monitor-cluster-creation","text":"You can observe tenant cluster creation from CCP Dashboard, however, if you are interested to see more details, you can login to vCenter and monitor VM cloning process.","title":"Monitor cluster creation"},{"location":"ccp_exploring/","text":"Exploring Cisco Container Platform Cisco Container Platform is a production grade platform to manage, monitor and deploy Kubernetes Clusters in your Enterprise. CCP uses 100% upstream Kuberenetes without vendor specific modification, creating seamless experience for developer to deploy application on any kubernetes platform in the public or private cloud. CCP provides authentication and authorization, security, high availability, networking, load balancing, and operational capabilities to effectively operate and manage Kubernetes clusters. CCP also provides a validated configuration of Kubernetes and can integrate with underlying infrastructure components such as Cisco HyperFlex and Cisco ACI. The infrastructure provider for CCP is Hyperflex. Cisco Container Platform has two main architecture components: Control Plane Cluster - to provide management platform for your Kubernetes Clusters where you can deploy new, scale worker nodes, manage policy and networking. The Control Plane is also build based on Kubernetes. Tenant Data Cluster - the Kubernetes cluster used to host applications across production, development, staging and many other environments Each user in this lab will have own Cisco Container Platform Control Plane. As described in the Lab task 1, check Table 1 with the URL to access your CCP Control Plane cluster dashboard. Explore CCP dashboard Login to your dedicated CCP Dashboard - find URL in Table 1 , use your Active Directory credentials that you can find in paper sheet on your desk. Once logged in, you will be taken to the \"cluster\" page. In this page you can manage your kubernetes clusters, edit their configuration, adding nodes or create node policies. In the left pane you will see other options such as User management , where users or group of users are managed locally or authentication could be integrated with Active Directory services. In our lab, Cisco Container Platform has been integrated with Active Directory. In the next menu position, you will see Infrastructure Providers . This is place where you select your cloud infrastructure, either it could be on-prem VMware vSphere or public cloud - Amazon Web Services. Cisco Container Platform requests resources such as virtual machines that later on will act as master and worker nodes of your Kubernetes cluster. Networks provides IP addressing subnets and pools configuration for the tenant clusters. When tenant cluster is deployed, the IP addresses for nodes are allocated from DHCP, however, Cisco Container Platform provides built-in mechanism to allocate IP addresses for Service Expousure under which applications will be exposed externally. The last tab contains Licensing , where you can register your Cisco Container Platform with Smart Licensing server.","title":"Explore Cisco Container Platform (CCP)"},{"location":"ccp_exploring/#exploring-cisco-container-platform","text":"Cisco Container Platform is a production grade platform to manage, monitor and deploy Kubernetes Clusters in your Enterprise. CCP uses 100% upstream Kuberenetes without vendor specific modification, creating seamless experience for developer to deploy application on any kubernetes platform in the public or private cloud. CCP provides authentication and authorization, security, high availability, networking, load balancing, and operational capabilities to effectively operate and manage Kubernetes clusters. CCP also provides a validated configuration of Kubernetes and can integrate with underlying infrastructure components such as Cisco HyperFlex and Cisco ACI. The infrastructure provider for CCP is Hyperflex. Cisco Container Platform has two main architecture components: Control Plane Cluster - to provide management platform for your Kubernetes Clusters where you can deploy new, scale worker nodes, manage policy and networking. The Control Plane is also build based on Kubernetes. Tenant Data Cluster - the Kubernetes cluster used to host applications across production, development, staging and many other environments Each user in this lab will have own Cisco Container Platform Control Plane. As described in the Lab task 1, check Table 1 with the URL to access your CCP Control Plane cluster dashboard.","title":"Exploring Cisco Container Platform"},{"location":"ccp_exploring/#explore-ccp-dashboard","text":"Login to your dedicated CCP Dashboard - find URL in Table 1 , use your Active Directory credentials that you can find in paper sheet on your desk. Once logged in, you will be taken to the \"cluster\" page. In this page you can manage your kubernetes clusters, edit their configuration, adding nodes or create node policies. In the left pane you will see other options such as User management , where users or group of users are managed locally or authentication could be integrated with Active Directory services. In our lab, Cisco Container Platform has been integrated with Active Directory. In the next menu position, you will see Infrastructure Providers . This is place where you select your cloud infrastructure, either it could be on-prem VMware vSphere or public cloud - Amazon Web Services. Cisco Container Platform requests resources such as virtual machines that later on will act as master and worker nodes of your Kubernetes cluster. Networks provides IP addressing subnets and pools configuration for the tenant clusters. When tenant cluster is deployed, the IP addresses for nodes are allocated from DHCP, however, Cisco Container Platform provides built-in mechanism to allocate IP addresses for Service Expousure under which applications will be exposed externally. The last tab contains Licensing , where you can register your Cisco Container Platform with Smart Licensing server.","title":"Explore CCP dashboard"},{"location":"ccp_grafana_kibana/","text":"Accessing Grafana dashboard Once you are logged in to Kubernetes cluster dashboard, you can obtain password to grafana dashboard which provides grafical view of Kubernetes cluster condition, but also to monitor your applications. Passwords are stored in Kubernetes Secrets object. Grafana admin password can be decoded from base64 encryption, and copy-pasted to grafana login page. Next steps will show you how to find grafana password: First, change namespace to CCP : Next, go to Secrets object in the menu, and look and the main pane on the right, you will be looking for secret called ccp-monitor-grafana Got to second page and there you will find desired Secret. In the Data field you will see admin-password and small eye icon. Please click on that icon to uncover the password. Once uncovered please copy it to clipboard. Alternatively, you can use following one-line kubectl command to obtain grafana admin-password. Please use this command from the master node, rather local PC as it may not have base64 installed. This command works only in linux environment, you can use it on the Kubernetes master node to which you can SSH kubectl -n ccp get secret ccp-monitor-grafana -o=jsonpath='{.data.admin-password}' | base64 --decode Next, please go back to the CCP dashboard, select your cluster, go into the detail mode and select Grafana button. You will be redirected to the Grafana page, where you can login with username admin and copied password from Secret. Once logged in to grafana, please select in the top left corner Home drop down menu - select Kubernetes Cluster Monitoring (Prometheus) . The dashboard where you can monitor resource utilisation of all PODs across all namespaces. You should see graphs like this - 7. Accessing logs on Cisco Container Platform The Elasticsearch, Fluentd, and Kibana (EFK) stack enables you to collect and monitor log data from containerized applications for troubleshooting or compliance purposes. These components are automatically installed when you install Cisco Container Platform. Fluentd is an open source data collector. It works at the backend to collect and forward the log data to Elasticsearch. Kibana is an open source analytics and visualization platform designed to work with Elasticsearch. It allows you to create rich visualizations and dashboards with the aggregated data. By default access to Kibana is not exposed due to security reasons. The quickest way to login to Kibana is to setup port-forwarding towards POD where kibana has been deployed. Use your installed kubectl CLI client on windows and enter following command: # kubectl get pods -n ccp NAME READY STATUS RESTARTS AGE ccp-efk-elasticsearch-curator-1548378000-7x2nd 0/1 Completed 0 2d ccp-efk-elasticsearch-curator-1548464400-9dzxv 0/1 Completed 0 1d ccp-efk-elasticsearch-curator-1548550800-gbv26 0/1 Completed 0 20h ccp-efk-kibana-6d7c97575c-gqjvg 1/1 Running 0 2d ccp-monitor-grafana-84d4f8b644-nqpdl 1/1 Running 0 2d ccp-monitor-grafana-set-datasource-llvlc 0/1 Completed 3 2d ccp-monitor-prometheus-alertmanager-64c4f944cb-zxc2c 2/2 Running 0 2d ccp-monitor-prometheus-kube-state-metrics-5b6855c558-lcpff 1/1 Running 0 2d ccp-monitor-prometheus-node-exporter-48hh2 1/1 Running 0 2d ccp-monitor-prometheus-node-exporter-9wbb9 1/1 Running 0 2d ccp-monitor-prometheus-pushgateway-7cfbcd8dfd-s625f 1/1 Running 0 2d ccp-monitor-prometheus-server-6fb9957f87-r7s29 2/2 Running 0 2d elasticsearch-logging-0 1/1 Running 0 2d fluentd-es-v2.0.2-8d6rm 1/1 Running 0 2d fluentd-es-v2.0.2-hljg5 1/1 Running 0 2d kubernetes-dashboard-5888c7c865-psk88 1/1 Running 0 2d metallb-controller-54559b4447-2rftf 1/1 Running 0 2d metallb-speaker-8pzdw 1/1 Running 0 2d metallb-speaker-rtbp4 1/1 Running 0 2d nginx-ingress-controller-fxg78 1/1 Running 0 2d nginx-ingress-controller-h8nk6 1/1 Running 0 2d nginx-ingress-default-backend-57fb689dd7-gx6sl 1/1 Running 0 2d kubectl port-foward -n ccp ccp-efk-kibana- id 5601:5601 example: kubectl port-foward -n ccp ccp-efk-kibana-6d7c97575c-gqjvg 5601:5601 Open Chrome web-browser, and enter following URL: https://localhost:5061 Once page will be opened, you will be asked to create index pattern. It means, you have to point from which log collection the logs will be presented. Click on Management and type logstash-* in the field - Next, you will be asked to add additional pattern to the presented logs After this, you will be able to see logs from all PODs and applications working on this cluster. To do this, go to Discovery panel, and type in the filter at the top of the page: kubernetes.namespace.name = default Since our application is deployed in default namespace, filter will show only logs from pods deployed in that namespace. Further filtering could be appliend, but this is not the main topic of this lab excercise.","title":"Explore Grafana and Kibana Modules on CCP"},{"location":"ccp_grafana_kibana/#accessing-grafana-dashboard","text":"Once you are logged in to Kubernetes cluster dashboard, you can obtain password to grafana dashboard which provides grafical view of Kubernetes cluster condition, but also to monitor your applications. Passwords are stored in Kubernetes Secrets object. Grafana admin password can be decoded from base64 encryption, and copy-pasted to grafana login page. Next steps will show you how to find grafana password: First, change namespace to CCP : Next, go to Secrets object in the menu, and look and the main pane on the right, you will be looking for secret called ccp-monitor-grafana Got to second page and there you will find desired Secret. In the Data field you will see admin-password and small eye icon. Please click on that icon to uncover the password. Once uncovered please copy it to clipboard. Alternatively, you can use following one-line kubectl command to obtain grafana admin-password. Please use this command from the master node, rather local PC as it may not have base64 installed. This command works only in linux environment, you can use it on the Kubernetes master node to which you can SSH kubectl -n ccp get secret ccp-monitor-grafana -o=jsonpath='{.data.admin-password}' | base64 --decode Next, please go back to the CCP dashboard, select your cluster, go into the detail mode and select Grafana button. You will be redirected to the Grafana page, where you can login with username admin and copied password from Secret. Once logged in to grafana, please select in the top left corner Home drop down menu - select Kubernetes Cluster Monitoring (Prometheus) . The dashboard where you can monitor resource utilisation of all PODs across all namespaces. You should see graphs like this -","title":"Accessing Grafana dashboard"},{"location":"ccp_grafana_kibana/#7-accessing-logs-on-cisco-container-platform","text":"The Elasticsearch, Fluentd, and Kibana (EFK) stack enables you to collect and monitor log data from containerized applications for troubleshooting or compliance purposes. These components are automatically installed when you install Cisco Container Platform. Fluentd is an open source data collector. It works at the backend to collect and forward the log data to Elasticsearch. Kibana is an open source analytics and visualization platform designed to work with Elasticsearch. It allows you to create rich visualizations and dashboards with the aggregated data. By default access to Kibana is not exposed due to security reasons. The quickest way to login to Kibana is to setup port-forwarding towards POD where kibana has been deployed. Use your installed kubectl CLI client on windows and enter following command: # kubectl get pods -n ccp NAME READY STATUS RESTARTS AGE ccp-efk-elasticsearch-curator-1548378000-7x2nd 0/1 Completed 0 2d ccp-efk-elasticsearch-curator-1548464400-9dzxv 0/1 Completed 0 1d ccp-efk-elasticsearch-curator-1548550800-gbv26 0/1 Completed 0 20h ccp-efk-kibana-6d7c97575c-gqjvg 1/1 Running 0 2d ccp-monitor-grafana-84d4f8b644-nqpdl 1/1 Running 0 2d ccp-monitor-grafana-set-datasource-llvlc 0/1 Completed 3 2d ccp-monitor-prometheus-alertmanager-64c4f944cb-zxc2c 2/2 Running 0 2d ccp-monitor-prometheus-kube-state-metrics-5b6855c558-lcpff 1/1 Running 0 2d ccp-monitor-prometheus-node-exporter-48hh2 1/1 Running 0 2d ccp-monitor-prometheus-node-exporter-9wbb9 1/1 Running 0 2d ccp-monitor-prometheus-pushgateway-7cfbcd8dfd-s625f 1/1 Running 0 2d ccp-monitor-prometheus-server-6fb9957f87-r7s29 2/2 Running 0 2d elasticsearch-logging-0 1/1 Running 0 2d fluentd-es-v2.0.2-8d6rm 1/1 Running 0 2d fluentd-es-v2.0.2-hljg5 1/1 Running 0 2d kubernetes-dashboard-5888c7c865-psk88 1/1 Running 0 2d metallb-controller-54559b4447-2rftf 1/1 Running 0 2d metallb-speaker-8pzdw 1/1 Running 0 2d metallb-speaker-rtbp4 1/1 Running 0 2d nginx-ingress-controller-fxg78 1/1 Running 0 2d nginx-ingress-controller-h8nk6 1/1 Running 0 2d nginx-ingress-default-backend-57fb689dd7-gx6sl 1/1 Running 0 2d kubectl port-foward -n ccp ccp-efk-kibana- id 5601:5601 example: kubectl port-foward -n ccp ccp-efk-kibana-6d7c97575c-gqjvg 5601:5601 Open Chrome web-browser, and enter following URL: https://localhost:5061 Once page will be opened, you will be asked to create index pattern. It means, you have to point from which log collection the logs will be presented. Click on Management and type logstash-* in the field - Next, you will be asked to add additional pattern to the presented logs After this, you will be able to see logs from all PODs and applications working on this cluster. To do this, go to Discovery panel, and type in the filter at the top of the page: kubernetes.namespace.name = default Since our application is deployed in default namespace, filter will show only logs from pods deployed in that namespace. Further filtering could be appliend, but this is not the main topic of this lab excercise.","title":"7. Accessing logs on Cisco Container Platform"},{"location":"ccp_modify_numer_of_nodes/","text":"Excercises: Cisco Container Platform - modify node configuration By default, when new cluster is created, it puts Master node and Worker nodes to separate pools of nodes. The reason behind it is to not deploy applications on the master node. This lab should be executed once your application has been deployed, on your Kubernetes cluster. One of the application components - REST APi agent is running on two worker nodes, since the replica has been set to 2. Let's try to remove one node from the cluster, and check how Kubernetes will maintain two copies of the POD. TASK 1 Using kubectl get information about all nodes in the cluster, including theis IP address. kubectl get pods -o wide ccpuser@pod06-a-ccp-data-master40bae43bca:~$ kubectl get pods -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE iot-backend-mariadb-6bbf6f4764-qxw6g 1/1 Running 0 1h 10.161.2.15 pod06-a-ccp-data-workerf49047e761 none iot-backend-mqtt-db-agent-57c88c58dd-wcbvs 1/1 Running 0 22m 10.161.2.21 pod06-a-ccp-data-workerf55c089976 none iot-backend-rest-api-agent-75bfb74dc4-bd52b 1/1 Running 0 22m 10.161.2.19 pod06-a-ccp-data-workerf49047e761 none iot-backend-rest-api-agent-75bfb74dc4-dgvbk 1/1 Running 0 34s 10.161.3.4 pod06-a-ccp-data-workerf55c089976 none TASK 2 Decrease number of worker nodes in your cluster to 1 Click on the small square next to default-pool to edit it's settings. Decrease number of nodes to one and save it may take several minutes until node will be removed, since Kubernetes must move PODs running on the removed node to the node that will stay within cluster. Sometimes the other node must download image, since that kind of POD was never deployed before TASK 3 Verify how PODs are distributed. Note how many replicas of iot-backend-rest-api-agent are running. Use kubectl command to check pods discritbution across worker nodes: kubectl get pods -o wide ccpuser@pod06-a-ccp-data-master40bae43bca:~$ kubectl get pods -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE iot-backend-mariadb-6bbf6f4764-qxw6g 1/1 Running 0 1h 10.161.2.15 pod06-a-ccp-data-workerf49047e761 none iot-backend-mqtt-db-agent-57c88c58dd-wcbvs 1/1 Running 0 22m 10.161.2.21 pod06-a-ccp-data-workerf49047e761 none iot-backend-rest-api-agent-75bfb74dc4-bd52b 1/1 Running 0 22m 10.161.2.19 pod06-a-ccp-data-workerf49047e761 none iot-backend-rest-api-agent-75bfb74dc4-dgvbk 1/1 Running 0 34s 10.161.3.4 pod06-a-ccp-data-workerf49047e761 none TASK 4 Increase number of nodes back to 2 . TASK 5 Verify how many replicas of iot-backend-rest-api-agent are running and on which worker nodes. TASK 6 Force Kuberenetes to reschedule and distribute iot-backend-rest-api-agent across nodes. Note that adding node to the cluster does not change distribution of PODs automatically. In order to force Kubernetes to deploy POD on the second node, please delete one of the POD iot-backend-rest-api-agent kubectl delete pod iot-backend-rest-api-agent-XXXXXX once this is completed verify what happend with kubectl get pods -o wide command. During delete operation, Kubernetes realized that POD should have 2 replicas, therefore, it deployed one more replica on the next worker node in the pool.","title":"Kubernetes Exercise"},{"location":"ccp_modify_numer_of_nodes/#excercises-cisco-container-platform-modify-node-configuration","text":"By default, when new cluster is created, it puts Master node and Worker nodes to separate pools of nodes. The reason behind it is to not deploy applications on the master node. This lab should be executed once your application has been deployed, on your Kubernetes cluster. One of the application components - REST APi agent is running on two worker nodes, since the replica has been set to 2. Let's try to remove one node from the cluster, and check how Kubernetes will maintain two copies of the POD. TASK 1 Using kubectl get information about all nodes in the cluster, including theis IP address. kubectl get pods -o wide ccpuser@pod06-a-ccp-data-master40bae43bca:~$ kubectl get pods -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE iot-backend-mariadb-6bbf6f4764-qxw6g 1/1 Running 0 1h 10.161.2.15 pod06-a-ccp-data-workerf49047e761 none iot-backend-mqtt-db-agent-57c88c58dd-wcbvs 1/1 Running 0 22m 10.161.2.21 pod06-a-ccp-data-workerf55c089976 none iot-backend-rest-api-agent-75bfb74dc4-bd52b 1/1 Running 0 22m 10.161.2.19 pod06-a-ccp-data-workerf49047e761 none iot-backend-rest-api-agent-75bfb74dc4-dgvbk 1/1 Running 0 34s 10.161.3.4 pod06-a-ccp-data-workerf55c089976 none TASK 2 Decrease number of worker nodes in your cluster to 1 Click on the small square next to default-pool to edit it's settings. Decrease number of nodes to one and save it may take several minutes until node will be removed, since Kubernetes must move PODs running on the removed node to the node that will stay within cluster. Sometimes the other node must download image, since that kind of POD was never deployed before TASK 3 Verify how PODs are distributed. Note how many replicas of iot-backend-rest-api-agent are running. Use kubectl command to check pods discritbution across worker nodes: kubectl get pods -o wide ccpuser@pod06-a-ccp-data-master40bae43bca:~$ kubectl get pods -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE iot-backend-mariadb-6bbf6f4764-qxw6g 1/1 Running 0 1h 10.161.2.15 pod06-a-ccp-data-workerf49047e761 none iot-backend-mqtt-db-agent-57c88c58dd-wcbvs 1/1 Running 0 22m 10.161.2.21 pod06-a-ccp-data-workerf49047e761 none iot-backend-rest-api-agent-75bfb74dc4-bd52b 1/1 Running 0 22m 10.161.2.19 pod06-a-ccp-data-workerf49047e761 none iot-backend-rest-api-agent-75bfb74dc4-dgvbk 1/1 Running 0 34s 10.161.3.4 pod06-a-ccp-data-workerf49047e761 none TASK 4 Increase number of nodes back to 2 . TASK 5 Verify how many replicas of iot-backend-rest-api-agent are running and on which worker nodes. TASK 6 Force Kuberenetes to reschedule and distribute iot-backend-rest-api-agent across nodes. Note that adding node to the cluster does not change distribution of PODs automatically. In order to force Kubernetes to deploy POD on the second node, please delete one of the POD iot-backend-rest-api-agent kubectl delete pod iot-backend-rest-api-agent-XXXXXX once this is completed verify what happend with kubectl get pods -o wide command. During delete operation, Kubernetes realized that POD should have 2 replicas, therefore, it deployed one more replica on the next worker node in the pool.","title":"Excercises: Cisco Container Platform - modify node configuration"},{"location":"create_gke_engine/","text":"Create Kubernetes Cluster (GKE) on Google Cloud Open Kubernetes Engine: 2 3 4 5 6","title":"Create Kubernetes Cluster (GKE) on Google Cloud"},{"location":"create_gke_engine/#create-kubernetes-cluster-gke-on-google-cloud","text":"","title":"Create Kubernetes Cluster (GKE) on Google Cloud"},{"location":"create_gke_engine/#open-kubernetes-engine","text":"","title":"Open Kubernetes Engine:"},{"location":"create_gke_engine/#2","text":"","title":"2"},{"location":"create_gke_engine/#3","text":"","title":"3"},{"location":"create_gke_engine/#4","text":"","title":"4"},{"location":"create_gke_engine/#5","text":"","title":"5"},{"location":"create_gke_engine/#6","text":"","title":"6"},{"location":"deploy_backend/","text":"Deploy the Backend Application Components on CCP Kubernetes Cluster (CCP Tenant Cluster) In this section you would deploy the backend components of the IoT Application on the Kubernetes cluster deployed on your CCP instance. Following diagram shows the high-level architecture of these backend application containers - Login to Kubernetes Master CLI Shell: SSH into the master node of the Kubernetes Cluster you deployed on top of CCP (Tenant Cluster) and start deploying the App's backend components by following the instruction on this page. The steps for logging into the Kubernetes Cluster (Tenant Cluster) CLI Shell are available at this link - Kubectl - Kubernetes Command Line Interface 1. Deploy MariaDB Databse: MariaDB will be used in the backend to save the sensor data received from AWS IoT platform over MQTT protocol. For this we would create following objects - Kubernetes Secret Kubernetes Persistent Volume Claim (PVC) Kubernetes MariaDB Deployment Kubernetes ClusterIP Service (Headless Service) Following diagram shows the relationship between these objects - 1.1 Create Kubernetes Secret for MariaDB: A Kubernetes Secret is an object that contains a small amount of sensitive data such as a password, a token, or a key. Such information might otherwise be put in a Pod specification or in an image; putting it in a Secret object allows for more control over how it is used, and reduces the risk of accidental exposure. The MariaDB container image uses an environment variable named as 'MYSQL_ROOT_PASSWORD', it hold the root password required to access the database. So you would create a new secret with 'password' key (value as 'cisco123') which would later be used in mariaDB deployment yaml file. 1.1.1: Create DB Password Secret - Use the following command to create a new secret on your kubernetes cluster - kubectl create secret generic mariadb-root-pass --from-literal=password=cisco123 1.1.2: Verify DB Password Secret - Check if the secret was created successfully or not - kubectl get secret mariadb-root-pass You should have the output similar to the following screenshot - 1.2 Create Kubernetes Persistent Volume Claim for MariaDB: A Kubernetes Persistent Volume Claim (PVC) is a request for storage by a user. It is similar to a pod. Pods consume node resources and PVCs consume Persistent Volume (PV) resources. Pods can request specific levels of resources (CPU and Memory). Claims can request specific size and access modes (e.g., can be mounted once read/write or many times read-only). To keep the sensor data safe during Pod restarts, you would create a new Persistent Volume Claim. The following yaml definition would be used to create the 'PersistentVolumeClaim' - --- apiVersion: v1 kind: PersistentVolumeClaim metadata: name: mariadb-pv-claim labels: app: iot-backend spec: accessModes: - ReadWriteOnce resources: requests: storage: 2Gi 1.2.1: Create Persistent Volume Claim - Use the following command to create a new Persistent Volume Claim for MariaDB Pod - kubectl create -f https://raw.githubusercontent.com/pradeesi/HybridCloudApp/master/HybridCloudApp/Kubernetes/Backend/Mariadb/mariadb_persistent_volume.yaml 1.2.2: Verify Persistent Volume Claim - Check if the PVC was created successfully or not - kubectl get pvc mariadb-pv-claim You should have the output similar to the following screenshot - Important: It can take up to a few minutes for the PVs to be provisioned. DO NOT procced futher till the PVC deployment gets completed. 1.3 Deploy MariaDB on Kubernetes: MariaDB is a community-developed fork of the MySQL relational database management system intended to remain free under the GNU GPL. Development is led by some of the original developers of MySQL, who forked it due to concerns over its acquisition by Oracle Corporation. MariaDB intends to maintain high compatibility with MySQL, ensuring a drop-in replacement capability with library binary parity and exact matching with MySQL APIs and commands. The following yaml definition will be used to deploy MariaDB pod - --- apiVersion: apps/v1 # for versions before 1.9.0 use apps/v1beta2 kind: Deployment metadata: name: iot-backend-mariadb labels: app: iot-backend spec: selector: matchLabels: app: iot-backend tier: mariadb strategy: type: Recreate template: metadata: labels: app: iot-backend tier: mariadb spec: containers: - image: mariadb:10.3 name: mariadb env: - name: MYSQL_ROOT_PASSWORD valueFrom: secretKeyRef: name: mariadb-root-pass key: password ports: - containerPort: 3306 name: mariadb volumeMounts: - name: mariadb-persistent-storage mountPath: /var/lib/mysql volumes: - name: mariadb-persistent-storage persistentVolumeClaim: claimName: mariadb-pv-claim 1.3.1: Deploy MariaDB - Use the following command to create a MariaDB kubernetes deployment - kubectl create -f https://raw.githubusercontent.com/pradeesi/HybridCloudApp/master/HybridCloudApp/Kubernetes/Backend/Mariadb/mariadb_deployment.yaml 1.3.1: Check Deployment Status - Use the following command to check if the kubernetes deployment was successfully created or not - kubectl get deployment iot-backend-mariadb You should have the output similar to the following screenshot - 1.3.2: Check Pod Status - Use the following command to check if the 'iot-backend-mariadb' pod is in ' Running ' state - kubectl get pods Note: Kubernetes may take some time to deploy the MariaDB. Do Not proceed further till the time DB Pod is up. 1.4 Create Kubernetes ClusterIP Service for MariaDB: Since the MariaDB will be accessed by other services like 'MQTT to DB Agent' and 'REST API Agent'; you need to expose it internally withing the kubernetes cluster using a Service using a Kubernetes 'ClusterIP' Service. A Kubernetes ClusterIP Service is the default Kubernetes service. It gives you a service inside your cluster that other apps inside your cluster can access. There is no external access (For testing you could access ClusterIP service via Kubernetes Proxy Service, though it is not recommended). Following yaml definition would be used to create the ClusterIP Service for MariaDB - --- apiVersion: v1 kind: Service metadata: name: mariadb-service labels: app: iot-backend spec: ports: - protocol: TCP port: 3306 targetPort: 3306 selector: app: iot-backend tier: mariadb clusterIP: None 1.4.1: Expose MariaDB to other Pods - Create a new kubernetes service using the following command - kubectl create -f https://raw.githubusercontent.com/pradeesi/HybridCloudApp/master/HybridCloudApp/Kubernetes/Backend/Mariadb/mariadb_service.yaml 1.4.2: Verify Service Status - Use the following command to check if the kubernetes service was deployed successfully or not - kubectl get service mariadb-service You should have the output similar to the following screenshot - 2. Deploy MQTT to DB Agent on Kubernetes: 'MQTT to DB Agent' will subscribe to the MQTT Topic and listen to the incoming sensor data from AWS IoT platform. It will then parse the sensor data and insert it into the MariaDB. The following yaml definition will be used to create the MQTT to DB Agent pods - --- apiVersion: apps/v1 # for versions before 1.9.0 use apps/v1beta2 kind: Deployment metadata: name: iot-backend-mqtt-db-agent labels: app: iot-backend tier: mqtt-db-agent spec: selector: matchLabels: app: iot-backend-mqtt-db-agent strategy: type: Recreate template: metadata: labels: app: iot-backend-mqtt-db-agent spec: containers: - image: pradeesi/mqtt_db_plugin:v2 name: mqtt-db-agent env: - name: DB_PASSWORD valueFrom: secretKeyRef: name: mariadb-root-pass key: password 2.1: Deploy MQTT to DB Agent - Use the following command to create mqtt-to-db-agent kubernetes deployment - kubectl create -f https://raw.githubusercontent.com/pradeesi/HybridCloudApp/master/HybridCloudApp/Kubernetes/Backend/MQTT_DB_Agent/mqtt_db_agent_deployment.yaml 2.2: Check Deployment Status - Use the following command to check if the kubernetes deployment was created successfully or not - kubectl get deployment iot-backend-mqtt-db-agent You should have the output similar to the following screenshot - 2.3: Check Pod Status - Use the following command to check if the 'iot-backend-mqtt-db-agent' pod is in ' Running ' state - kubectl get pods Note: You may check the Pod Logs using the command ' kubectl logs \\ pod_name> ' 3. Deploy REST API Agent on Kubernetes: The 'REST API Agent' would act as the gateway to the backend application. It will listen to the incoming HTTP requests from the frontend application that you will deploy on Google Cloud. 3.1 Deploy REST API Agent: The following yaml definition will be used to create REST API Agent pods - --- apiVersion: apps/v1 # for versions before 1.9.0 use apps/v1beta2 kind: Deployment metadata: name: iot-backend-rest-api-agent labels: app: iot-backend-rest-api-agent spec: replicas: 1 selector: matchLabels: app: iot-backend-rest-api-agent tier: rest-api-agent strategy: type: Recreate template: metadata: labels: app: iot-backend-rest-api-agent tier: rest-api-agent spec: containers: - image: pradeesi/rest_api_agent:v1 name: rest-api-agent env: - name: DB_PASSWORD valueFrom: secretKeyRef: name: mariadb-root-pass key: password 3.1.1: Deploy REST API Agent - Use the following command to create the rest-api-agent kubernetes deployment - kubectl create -f https://raw.githubusercontent.com/pradeesi/HybridCloudApp/master/HybridCloudApp/Kubernetes/Backend/REST_API_Agent/rest_api_agent.yaml 3.1.2: Check Deployment Status - Use the following command to check if the kubernetes deployment was created successfully or not - kubectl get deployment iot-backend-rest-api-agent You should have the output similar to the following screenshot - 3.1.3: Check Pod Status - Use the following command to check if the 'iot-backend-rest-api-agent' pod is in ' Running ' state - kubectl get pods Note: You may check the Pod Logs using the command ' kubectl logs \\ pod_name> ' 3.2 Create Kubernetes NodePort Service for REST API Agent: Since the frontend app from Google Cloud would access the REST APIs exposed by the 'REST API Agent', you need to create a new kubernetes service for it. The following yaml definition would be used for to create a NodePort Service for the REST API Agent - --- apiVersion: v1 kind: Service metadata: name: rest-api-agent-service labels: app: iot-backend spec: ports: - protocol: TCP port: 5050 nodePort: 30500 selector: app: iot-backend-rest-api-agent tier: rest-api-agent type: \"NodePort\" 3.2.1: Create REST API Agent NodePort Service - You can create a new kubernetes service using the following command - kubectl create -f https://raw.githubusercontent.com/pradeesi/HybridCloudApp/master/HybridCloudApp/Kubernetes/Backend/REST_API_Agent/rest_api_agent_service_node_port.yaml 3.2.2: Check REST API Agent Service Status - You can use the following command to check if the kubernetes service was created successfully or not - kubectl get service rest-api-agent-service You should have the output similar to the following screenshot - 3.3 Locate the IP and Port to Access Node-Port Service for REST API Agent: You need to find the NodePort and Kubernetes Node external IP to access the 'rest-api-agent. Use the following command to display the port exposed by 'rest-api-agent-service' - kubectl get service rest-api-agent-service Use the following command to display the 'External-IP' of you kubernetes nodes - kubectl get nodes -o wide Following screenshot highlights the Port and Node IPs in the command outputs - Important: Note down the Node External IP Address and NodePort Service Port Number. These values would be used in next section for deploying the frontend app as the environment variables values (' BACKEND_HOST ' and ' BACKEND_PORT '). 4 Test the REST APIs Exposed by REST API Agent Service: To test the REST API service try to access following url from your web browser (use the node's external ip and service port from the previous section # 3.3)- http:// kubernetes node's external ip :30500/ If your REST API Agent is working properly, you should see 'Welcome to the API Service...!' message on your browser as shown in the following screenshot - Following are the other urls that you could test - http:// kubernetes node's external ip :30500/cities http:// kubernetes node's external ip :30500/temperature http:// kubernetes node's external ip :30500/humidity http:// kubernetes node's external ip :30500/sensor_data/city","title":"Deploy Backend App on Kubernetes Cluster (Tenant Cluster)"},{"location":"deploy_backend/#deploy-the-backend-application-components-on-ccp-kubernetes-cluster-ccp-tenant-cluster","text":"In this section you would deploy the backend components of the IoT Application on the Kubernetes cluster deployed on your CCP instance. Following diagram shows the high-level architecture of these backend application containers -","title":"Deploy the Backend Application Components on CCP Kubernetes Cluster (CCP Tenant Cluster)"},{"location":"deploy_backend/#login-to-kubernetes-master-cli-shell","text":"SSH into the master node of the Kubernetes Cluster you deployed on top of CCP (Tenant Cluster) and start deploying the App's backend components by following the instruction on this page. The steps for logging into the Kubernetes Cluster (Tenant Cluster) CLI Shell are available at this link - Kubectl - Kubernetes Command Line Interface","title":"Login to Kubernetes Master CLI Shell:"},{"location":"deploy_backend/#1-deploy-mariadb-databse","text":"MariaDB will be used in the backend to save the sensor data received from AWS IoT platform over MQTT protocol. For this we would create following objects - Kubernetes Secret Kubernetes Persistent Volume Claim (PVC) Kubernetes MariaDB Deployment Kubernetes ClusterIP Service (Headless Service) Following diagram shows the relationship between these objects -","title":"1. Deploy MariaDB Databse:"},{"location":"deploy_backend/#11-create-kubernetes-secret-for-mariadb","text":"A Kubernetes Secret is an object that contains a small amount of sensitive data such as a password, a token, or a key. Such information might otherwise be put in a Pod specification or in an image; putting it in a Secret object allows for more control over how it is used, and reduces the risk of accidental exposure. The MariaDB container image uses an environment variable named as 'MYSQL_ROOT_PASSWORD', it hold the root password required to access the database. So you would create a new secret with 'password' key (value as 'cisco123') which would later be used in mariaDB deployment yaml file. 1.1.1: Create DB Password Secret - Use the following command to create a new secret on your kubernetes cluster - kubectl create secret generic mariadb-root-pass --from-literal=password=cisco123 1.1.2: Verify DB Password Secret - Check if the secret was created successfully or not - kubectl get secret mariadb-root-pass You should have the output similar to the following screenshot -","title":"1.1 Create Kubernetes Secret for MariaDB:"},{"location":"deploy_backend/#12-create-kubernetes-persistent-volume-claim-for-mariadb","text":"A Kubernetes Persistent Volume Claim (PVC) is a request for storage by a user. It is similar to a pod. Pods consume node resources and PVCs consume Persistent Volume (PV) resources. Pods can request specific levels of resources (CPU and Memory). Claims can request specific size and access modes (e.g., can be mounted once read/write or many times read-only). To keep the sensor data safe during Pod restarts, you would create a new Persistent Volume Claim. The following yaml definition would be used to create the 'PersistentVolumeClaim' - --- apiVersion: v1 kind: PersistentVolumeClaim metadata: name: mariadb-pv-claim labels: app: iot-backend spec: accessModes: - ReadWriteOnce resources: requests: storage: 2Gi 1.2.1: Create Persistent Volume Claim - Use the following command to create a new Persistent Volume Claim for MariaDB Pod - kubectl create -f https://raw.githubusercontent.com/pradeesi/HybridCloudApp/master/HybridCloudApp/Kubernetes/Backend/Mariadb/mariadb_persistent_volume.yaml 1.2.2: Verify Persistent Volume Claim - Check if the PVC was created successfully or not - kubectl get pvc mariadb-pv-claim You should have the output similar to the following screenshot - Important: It can take up to a few minutes for the PVs to be provisioned. DO NOT procced futher till the PVC deployment gets completed.","title":"1.2 Create Kubernetes Persistent Volume Claim for MariaDB:"},{"location":"deploy_backend/#13-deploy-mariadb-on-kubernetes","text":"MariaDB is a community-developed fork of the MySQL relational database management system intended to remain free under the GNU GPL. Development is led by some of the original developers of MySQL, who forked it due to concerns over its acquisition by Oracle Corporation. MariaDB intends to maintain high compatibility with MySQL, ensuring a drop-in replacement capability with library binary parity and exact matching with MySQL APIs and commands. The following yaml definition will be used to deploy MariaDB pod - --- apiVersion: apps/v1 # for versions before 1.9.0 use apps/v1beta2 kind: Deployment metadata: name: iot-backend-mariadb labels: app: iot-backend spec: selector: matchLabels: app: iot-backend tier: mariadb strategy: type: Recreate template: metadata: labels: app: iot-backend tier: mariadb spec: containers: - image: mariadb:10.3 name: mariadb env: - name: MYSQL_ROOT_PASSWORD valueFrom: secretKeyRef: name: mariadb-root-pass key: password ports: - containerPort: 3306 name: mariadb volumeMounts: - name: mariadb-persistent-storage mountPath: /var/lib/mysql volumes: - name: mariadb-persistent-storage persistentVolumeClaim: claimName: mariadb-pv-claim 1.3.1: Deploy MariaDB - Use the following command to create a MariaDB kubernetes deployment - kubectl create -f https://raw.githubusercontent.com/pradeesi/HybridCloudApp/master/HybridCloudApp/Kubernetes/Backend/Mariadb/mariadb_deployment.yaml 1.3.1: Check Deployment Status - Use the following command to check if the kubernetes deployment was successfully created or not - kubectl get deployment iot-backend-mariadb You should have the output similar to the following screenshot - 1.3.2: Check Pod Status - Use the following command to check if the 'iot-backend-mariadb' pod is in ' Running ' state - kubectl get pods Note: Kubernetes may take some time to deploy the MariaDB. Do Not proceed further till the time DB Pod is up.","title":"1.3 Deploy MariaDB on Kubernetes:"},{"location":"deploy_backend/#14-create-kubernetes-clusterip-service-for-mariadb","text":"Since the MariaDB will be accessed by other services like 'MQTT to DB Agent' and 'REST API Agent'; you need to expose it internally withing the kubernetes cluster using a Service using a Kubernetes 'ClusterIP' Service. A Kubernetes ClusterIP Service is the default Kubernetes service. It gives you a service inside your cluster that other apps inside your cluster can access. There is no external access (For testing you could access ClusterIP service via Kubernetes Proxy Service, though it is not recommended). Following yaml definition would be used to create the ClusterIP Service for MariaDB - --- apiVersion: v1 kind: Service metadata: name: mariadb-service labels: app: iot-backend spec: ports: - protocol: TCP port: 3306 targetPort: 3306 selector: app: iot-backend tier: mariadb clusterIP: None 1.4.1: Expose MariaDB to other Pods - Create a new kubernetes service using the following command - kubectl create -f https://raw.githubusercontent.com/pradeesi/HybridCloudApp/master/HybridCloudApp/Kubernetes/Backend/Mariadb/mariadb_service.yaml 1.4.2: Verify Service Status - Use the following command to check if the kubernetes service was deployed successfully or not - kubectl get service mariadb-service You should have the output similar to the following screenshot -","title":"1.4 Create Kubernetes ClusterIP Service for MariaDB:"},{"location":"deploy_backend/#2-deploy-mqtt-to-db-agent-on-kubernetes","text":"'MQTT to DB Agent' will subscribe to the MQTT Topic and listen to the incoming sensor data from AWS IoT platform. It will then parse the sensor data and insert it into the MariaDB. The following yaml definition will be used to create the MQTT to DB Agent pods - --- apiVersion: apps/v1 # for versions before 1.9.0 use apps/v1beta2 kind: Deployment metadata: name: iot-backend-mqtt-db-agent labels: app: iot-backend tier: mqtt-db-agent spec: selector: matchLabels: app: iot-backend-mqtt-db-agent strategy: type: Recreate template: metadata: labels: app: iot-backend-mqtt-db-agent spec: containers: - image: pradeesi/mqtt_db_plugin:v2 name: mqtt-db-agent env: - name: DB_PASSWORD valueFrom: secretKeyRef: name: mariadb-root-pass key: password 2.1: Deploy MQTT to DB Agent - Use the following command to create mqtt-to-db-agent kubernetes deployment - kubectl create -f https://raw.githubusercontent.com/pradeesi/HybridCloudApp/master/HybridCloudApp/Kubernetes/Backend/MQTT_DB_Agent/mqtt_db_agent_deployment.yaml 2.2: Check Deployment Status - Use the following command to check if the kubernetes deployment was created successfully or not - kubectl get deployment iot-backend-mqtt-db-agent You should have the output similar to the following screenshot - 2.3: Check Pod Status - Use the following command to check if the 'iot-backend-mqtt-db-agent' pod is in ' Running ' state - kubectl get pods Note: You may check the Pod Logs using the command ' kubectl logs \\ pod_name> '","title":"2. Deploy MQTT to DB Agent on Kubernetes:"},{"location":"deploy_backend/#3-deploy-rest-api-agent-on-kubernetes","text":"The 'REST API Agent' would act as the gateway to the backend application. It will listen to the incoming HTTP requests from the frontend application that you will deploy on Google Cloud.","title":"3. Deploy REST API Agent on Kubernetes:"},{"location":"deploy_backend/#31-deploy-rest-api-agent","text":"The following yaml definition will be used to create REST API Agent pods - --- apiVersion: apps/v1 # for versions before 1.9.0 use apps/v1beta2 kind: Deployment metadata: name: iot-backend-rest-api-agent labels: app: iot-backend-rest-api-agent spec: replicas: 1 selector: matchLabels: app: iot-backend-rest-api-agent tier: rest-api-agent strategy: type: Recreate template: metadata: labels: app: iot-backend-rest-api-agent tier: rest-api-agent spec: containers: - image: pradeesi/rest_api_agent:v1 name: rest-api-agent env: - name: DB_PASSWORD valueFrom: secretKeyRef: name: mariadb-root-pass key: password 3.1.1: Deploy REST API Agent - Use the following command to create the rest-api-agent kubernetes deployment - kubectl create -f https://raw.githubusercontent.com/pradeesi/HybridCloudApp/master/HybridCloudApp/Kubernetes/Backend/REST_API_Agent/rest_api_agent.yaml 3.1.2: Check Deployment Status - Use the following command to check if the kubernetes deployment was created successfully or not - kubectl get deployment iot-backend-rest-api-agent You should have the output similar to the following screenshot - 3.1.3: Check Pod Status - Use the following command to check if the 'iot-backend-rest-api-agent' pod is in ' Running ' state - kubectl get pods Note: You may check the Pod Logs using the command ' kubectl logs \\ pod_name> '","title":"3.1 Deploy REST API Agent:"},{"location":"deploy_backend/#32-create-kubernetes-nodeport-service-for-rest-api-agent","text":"Since the frontend app from Google Cloud would access the REST APIs exposed by the 'REST API Agent', you need to create a new kubernetes service for it. The following yaml definition would be used for to create a NodePort Service for the REST API Agent - --- apiVersion: v1 kind: Service metadata: name: rest-api-agent-service labels: app: iot-backend spec: ports: - protocol: TCP port: 5050 nodePort: 30500 selector: app: iot-backend-rest-api-agent tier: rest-api-agent type: \"NodePort\" 3.2.1: Create REST API Agent NodePort Service - You can create a new kubernetes service using the following command - kubectl create -f https://raw.githubusercontent.com/pradeesi/HybridCloudApp/master/HybridCloudApp/Kubernetes/Backend/REST_API_Agent/rest_api_agent_service_node_port.yaml 3.2.2: Check REST API Agent Service Status - You can use the following command to check if the kubernetes service was created successfully or not - kubectl get service rest-api-agent-service You should have the output similar to the following screenshot -","title":"3.2 Create Kubernetes NodePort Service for REST API Agent:"},{"location":"deploy_backend/#33-locate-the-ip-and-port-to-access-node-port-service-for-rest-api-agent","text":"You need to find the NodePort and Kubernetes Node external IP to access the 'rest-api-agent. Use the following command to display the port exposed by 'rest-api-agent-service' - kubectl get service rest-api-agent-service Use the following command to display the 'External-IP' of you kubernetes nodes - kubectl get nodes -o wide Following screenshot highlights the Port and Node IPs in the command outputs - Important: Note down the Node External IP Address and NodePort Service Port Number. These values would be used in next section for deploying the frontend app as the environment variables values (' BACKEND_HOST ' and ' BACKEND_PORT ').","title":"3.3 Locate the IP and Port to Access Node-Port Service for REST API Agent:"},{"location":"deploy_backend/#4-test-the-rest-apis-exposed-by-rest-api-agent-service","text":"To test the REST API service try to access following url from your web browser (use the node's external ip and service port from the previous section # 3.3)- http:// kubernetes node's external ip :30500/ If your REST API Agent is working properly, you should see 'Welcome to the API Service...!' message on your browser as shown in the following screenshot - Following are the other urls that you could test - http:// kubernetes node's external ip :30500/cities http:// kubernetes node's external ip :30500/temperature http:// kubernetes node's external ip :30500/humidity http:// kubernetes node's external ip :30500/sensor_data/city","title":"4 Test the REST APIs Exposed by REST API Agent Service:"},{"location":"deploy_frontend/","text":"Deploy the Frontend Application Components on Google Kubernetes Engine (GKE) In this section you would deploy the frontend components of the IoT Application on the Google Kubernetes Engine. Following diagram shows the high-level architecture of these frontend application containers - 1. Login to Google Cloud Console and Open Kubernetes Engine: Login to Google Cloud Console using the credentials provided by the lab instructor. You can find the details of login method at this link - Google Cloud Access 2. Creating Deployment Definition using GKE UI on Google Cloud Console: You will create kubernetes deployment for frontend app and expose it to the internet using Kubernetes Load Balancer Service as shown in the following diagram - 2.1: Select the ' Workloads ' option on \" Google Cloud Console -- Kubernetes Engine \" page, and click on the ' Deploy ' button as shown in the following screenshot - 3. Add 'frontend_server' Container Image to the Deployment Definition: 3.1: Select the ' Existing container image ' radio button on the 'Create Deployment' page and then click on 'SELECT' button to select the image as shown in the following screenshot - 3.2: Select the ' frontend_server ' container image from the pop-up window and click on the ' SELECT ' button as shown in the following screenshot - 4. Add Environment Variables to the 'frontend_server' Container: 4.1: Click on the '+ Add environment variable' button to add the environment variables for the 'frontend_server' container as shown in the following screenshot - 4.2: Add ' BACKEND_HOST ' and ' BACKEND_PORT ' variables as shown in the following screenshot (Use the values for 'BACKEND_HOST' and 'BACKEND_PORT' from the REST API Agent NodePort Service created earlier) - 5. Add Second Container Image ('nginx_srvr') to the Deployment Definition: 5.1: After clicking on the ' + Add Container ' button (shown in the previous screenshot), click again on the ' Existing ontainer image ' and click on the ' SELECT ' button. It will display the popup window. For this popup window, select the ' nginx_srvr ' image with 'latest' tag. 5.2: Click on the 'CONTINUE' button as shown in the following screenshot - 6. Add Application Name, Select Cluster and Deploy the Application: 6.1: Change the application name to ' iot-frontend-user-X ' (Replace X with you POD number) and select the 'Cluster' from the drop down menu. Now you can click on the 'Deploy' button as shown in the following screenshot - Importnat: Application deployment may take some time. Wait for it's completion before proceeding with the next steps. 7. Expose the Application by Creating Kubernetes Service: Click on ' Workloads ' option from the left panel on the GKE Dashboard. 7.1: Click on your Workload name ' iot-frontend-user-X ' (Kubernetes Deployment) as shown in the following screenshot - 7.2: Click on the ' Deploy ' button as shown in the following screenshot - 7.3: In the 'New Port Mapping' set ' Port ' and ' Target Port ' as ' 80 ' and click on the ' Done ' button. Make sure the ' Service type ' is ' Load balancer ' and click on the ' Expose ' button as shown in the following screenshot - Important: Service deployment may take some time. Wait for it before proceeding with the next steps. 8. Open the Application Dashboard: 8.1: Go to ' Kubernetes Engine -- Services ' and click on the ' Endpoints ' respective to your kubernetes service as shown in the following screenshot - 8.2: You should see the following webpage in the new tab of your browser. Click on the \"Open Dashboard\" button to open the application dashboard as shown in the following screenshot - 8.3: If you see the following web-page with charts filled with data, your application is working :-) Congratulations!!!","title":"Deploy Frontend App on Google Kubernetes Engine (GKE)"},{"location":"deploy_frontend/#deploy-the-frontend-application-components-on-google-kubernetes-engine-gke","text":"In this section you would deploy the frontend components of the IoT Application on the Google Kubernetes Engine. Following diagram shows the high-level architecture of these frontend application containers -","title":"Deploy the Frontend Application Components on Google Kubernetes Engine (GKE)"},{"location":"deploy_frontend/#1-login-to-google-cloud-console-and-open-kubernetes-engine","text":"Login to Google Cloud Console using the credentials provided by the lab instructor. You can find the details of login method at this link - Google Cloud Access","title":"1. Login to Google Cloud Console and Open Kubernetes Engine:"},{"location":"deploy_frontend/#2-creating-deployment-definition-using-gke-ui-on-google-cloud-console","text":"You will create kubernetes deployment for frontend app and expose it to the internet using Kubernetes Load Balancer Service as shown in the following diagram - 2.1: Select the ' Workloads ' option on \" Google Cloud Console -- Kubernetes Engine \" page, and click on the ' Deploy ' button as shown in the following screenshot -","title":"2. Creating Deployment Definition using GKE UI on Google Cloud Console:"},{"location":"deploy_frontend/#3-add-frontend_server-container-image-to-the-deployment-definition","text":"3.1: Select the ' Existing container image ' radio button on the 'Create Deployment' page and then click on 'SELECT' button to select the image as shown in the following screenshot - 3.2: Select the ' frontend_server ' container image from the pop-up window and click on the ' SELECT ' button as shown in the following screenshot -","title":"3. Add 'frontend_server' Container Image to the Deployment Definition:"},{"location":"deploy_frontend/#4-add-environment-variables-to-the-frontend_server-container","text":"4.1: Click on the '+ Add environment variable' button to add the environment variables for the 'frontend_server' container as shown in the following screenshot - 4.2: Add ' BACKEND_HOST ' and ' BACKEND_PORT ' variables as shown in the following screenshot (Use the values for 'BACKEND_HOST' and 'BACKEND_PORT' from the REST API Agent NodePort Service created earlier) -","title":"4. Add Environment Variables to the 'frontend_server' Container:"},{"location":"deploy_frontend/#5-add-second-container-image-nginx_srvr-to-the-deployment-definition","text":"5.1: After clicking on the ' + Add Container ' button (shown in the previous screenshot), click again on the ' Existing ontainer image ' and click on the ' SELECT ' button. It will display the popup window. For this popup window, select the ' nginx_srvr ' image with 'latest' tag. 5.2: Click on the 'CONTINUE' button as shown in the following screenshot -","title":"5. Add Second Container Image ('nginx_srvr') to the Deployment Definition:"},{"location":"deploy_frontend/#6-add-application-name-select-cluster-and-deploy-the-application","text":"6.1: Change the application name to ' iot-frontend-user-X ' (Replace X with you POD number) and select the 'Cluster' from the drop down menu. Now you can click on the 'Deploy' button as shown in the following screenshot - Importnat: Application deployment may take some time. Wait for it's completion before proceeding with the next steps.","title":"6. Add Application Name, Select Cluster and Deploy the Application:"},{"location":"deploy_frontend/#7-expose-the-application-by-creating-kubernetes-service","text":"Click on ' Workloads ' option from the left panel on the GKE Dashboard. 7.1: Click on your Workload name ' iot-frontend-user-X ' (Kubernetes Deployment) as shown in the following screenshot - 7.2: Click on the ' Deploy ' button as shown in the following screenshot - 7.3: In the 'New Port Mapping' set ' Port ' and ' Target Port ' as ' 80 ' and click on the ' Done ' button. Make sure the ' Service type ' is ' Load balancer ' and click on the ' Expose ' button as shown in the following screenshot - Important: Service deployment may take some time. Wait for it before proceeding with the next steps.","title":"7. Expose the Application by Creating Kubernetes Service:"},{"location":"deploy_frontend/#8-open-the-application-dashboard","text":"8.1: Go to ' Kubernetes Engine -- Services ' and click on the ' Endpoints ' respective to your kubernetes service as shown in the following screenshot - 8.2: You should see the following webpage in the new tab of your browser. Click on the \"Open Dashboard\" button to open the application dashboard as shown in the following screenshot - 8.3: If you see the following web-page with charts filled with data, your application is working :-) Congratulations!!!","title":"8. Open the Application Dashboard:"},{"location":"kubernetes_basics/","text":"Appendix - 3: Kubernetes Basic Docs 1. Install and Configure kubectl For information on \"kubectl\" installation and configuration use the following document - https://kubernetes.io/docs/tasks/tools/install-kubectl/ 2. Google Cloud Container Registry: Pushing and Pulling Container Images To push or pull the container images to/from Google Container Registry you can follow the following article - https://cloud.google.com/container-registry/docs/pushing-and-pulling Note: You need to login to your Google Cloud account before pushing the container image. You can do it using the following command - gcloud auth login If you have multiple projects, you can select a specific one using the following command - gcloud config set project Project_ID Kubernetes Cheat Sheet: https://kubernetes.io/docs/reference/kubectl/cheatsheet/","title":"Appendix - 3: Kubernetes Basic Docs"},{"location":"kubernetes_basics/#appendix-3-kubernetes-basic-docs","text":"","title":"Appendix - 3: Kubernetes Basic Docs"},{"location":"kubernetes_basics/#1-install-and-configure-kubectl","text":"For information on \"kubectl\" installation and configuration use the following document - https://kubernetes.io/docs/tasks/tools/install-kubectl/","title":"1. Install and Configure kubectl"},{"location":"kubernetes_basics/#2-google-cloud-container-registry-pushing-and-pulling-container-images","text":"To push or pull the container images to/from Google Container Registry you can follow the following article - https://cloud.google.com/container-registry/docs/pushing-and-pulling Note: You need to login to your Google Cloud account before pushing the container image. You can do it using the following command - gcloud auth login If you have multiple projects, you can select a specific one using the following command - gcloud config set project Project_ID","title":"2. Google Cloud Container Registry: Pushing and Pulling Container Images"},{"location":"kubernetes_basics/#kubernetes-cheat-sheet","text":"https://kubernetes.io/docs/reference/kubectl/cheatsheet/","title":"Kubernetes Cheat Sheet:"},{"location":"security_policies/","text":"Applying Kubernetes Network Policies to Secure the Application A network policy is a specification of how groups of pods are allowed to communicate with each other and other network endpoints. NetworkPolicy resources use labels to select pods and define rules which specify what traffic is allowed to the selected pods. By now you should have your Hybrid Cloud IoT Application working end to end. Let's apply some Kubernetes Network Policies to allow traffic from the frontend app to port '5050' only (REST API AGENT container accepts HTTP requests on port '5050'). 1. Apply Deny All Network Policy: Following Kubernetes Network Policy yaml definition would block all the traffic coming towards REST API Agent - apiVersion: networking.k8s.io/v1 kind: NetworkPolicy metadata: name: deny-all-rest-api-agent namespace: default spec: podSelector: matchLabels: app: iot-backend-rest-api-agent tier: rest-api-agent policyTypes: - Ingress 1.1: Execute the following command on Kubernetes master node to apply this Network Policy on your REST API Agent - kubectl create -f https://raw.githubusercontent.com/pradeesi/HybridCloudApp/master/HybridCloudApp/Kubernetes/Backend/Network_Policies/deny_all_rest_api_agent.yaml 1.2: Now try to referesh your Frontend App webpage. It should stop working. 2. Apply Permit Port 5111 Network Policy: Following Kubernetes Network Policy yaml definition would allow the traffic on port 5111 to REST API Agent - apiVersion: networking.k8s.io/v1 kind: NetworkPolicy metadata: name: permit-port-5111-rest-api-agent namespace: default spec: podSelector: matchLabels: app: iot-backend-rest-api-agent tier: rest-api-agent policyTypes: - Ingress ingress: - from: [] ports: - protocol: TCP port: 5111 2.1: Execute the following command on Kubernetes master node to apply this Network Policy on your REST API Agent - kubectl create -f https://raw.githubusercontent.com/pradeesi/HybridCloudApp/master/HybridCloudApp/Kubernetes/Backend/Network_Policies/permit_port_5111_rest_api_agent.yaml 2.2: Now try to referesh your Frontend App webpage. Does it work? Why? 3. Apply Permit Port 5050 Network Policy: Following Kubernetes Network Policy yaml definition would allow the traffic on port 5050 to REST API Agent - apiVersion: networking.k8s.io/v1 kind: NetworkPolicy metadata: name: permit-port-5050-rest-api-agent namespace: default spec: podSelector: matchLabels: app: iot-backend-rest-api-agent tier: rest-api-agent policyTypes: - Ingress ingress: - from: [] ports: - protocol: TCP port: 5050 3.1: Execute the following command on Kubernetes master node to apply this Network Policy on your REST API Agent - kubectl create -f https://raw.githubusercontent.com/pradeesi/HybridCloudApp/master/HybridCloudApp/Kubernetes/Backend/Network_Policies/permit_port_5050_rest_api_agent.yaml 3.2: Now try to referesh your Frontend App webpage. Does it work? Why? Note: Other command related to Network Policy that you may use - Display Kubernetes Network Policies - kubectl get NetworkPolicies Display Network Policy Details - kubectl describe NetworkPolicy Network Policy Name Delete Network Policy - kubectl delete NetworkPolicy Network Policy Name","title":"Apply Security Policy to REST API Agent"},{"location":"security_policies/#applying-kubernetes-network-policies-to-secure-the-application","text":"A network policy is a specification of how groups of pods are allowed to communicate with each other and other network endpoints. NetworkPolicy resources use labels to select pods and define rules which specify what traffic is allowed to the selected pods. By now you should have your Hybrid Cloud IoT Application working end to end. Let's apply some Kubernetes Network Policies to allow traffic from the frontend app to port '5050' only (REST API AGENT container accepts HTTP requests on port '5050').","title":"Applying Kubernetes Network Policies to Secure the Application"},{"location":"security_policies/#1-apply-deny-all-network-policy","text":"Following Kubernetes Network Policy yaml definition would block all the traffic coming towards REST API Agent - apiVersion: networking.k8s.io/v1 kind: NetworkPolicy metadata: name: deny-all-rest-api-agent namespace: default spec: podSelector: matchLabels: app: iot-backend-rest-api-agent tier: rest-api-agent policyTypes: - Ingress 1.1: Execute the following command on Kubernetes master node to apply this Network Policy on your REST API Agent - kubectl create -f https://raw.githubusercontent.com/pradeesi/HybridCloudApp/master/HybridCloudApp/Kubernetes/Backend/Network_Policies/deny_all_rest_api_agent.yaml 1.2: Now try to referesh your Frontend App webpage. It should stop working.","title":"1. Apply Deny All Network Policy:"},{"location":"security_policies/#2-apply-permit-port-5111-network-policy","text":"Following Kubernetes Network Policy yaml definition would allow the traffic on port 5111 to REST API Agent - apiVersion: networking.k8s.io/v1 kind: NetworkPolicy metadata: name: permit-port-5111-rest-api-agent namespace: default spec: podSelector: matchLabels: app: iot-backend-rest-api-agent tier: rest-api-agent policyTypes: - Ingress ingress: - from: [] ports: - protocol: TCP port: 5111 2.1: Execute the following command on Kubernetes master node to apply this Network Policy on your REST API Agent - kubectl create -f https://raw.githubusercontent.com/pradeesi/HybridCloudApp/master/HybridCloudApp/Kubernetes/Backend/Network_Policies/permit_port_5111_rest_api_agent.yaml 2.2: Now try to referesh your Frontend App webpage. Does it work? Why?","title":"2. Apply Permit Port 5111 Network Policy:"},{"location":"security_policies/#3-apply-permit-port-5050-network-policy","text":"Following Kubernetes Network Policy yaml definition would allow the traffic on port 5050 to REST API Agent - apiVersion: networking.k8s.io/v1 kind: NetworkPolicy metadata: name: permit-port-5050-rest-api-agent namespace: default spec: podSelector: matchLabels: app: iot-backend-rest-api-agent tier: rest-api-agent policyTypes: - Ingress ingress: - from: [] ports: - protocol: TCP port: 5050 3.1: Execute the following command on Kubernetes master node to apply this Network Policy on your REST API Agent - kubectl create -f https://raw.githubusercontent.com/pradeesi/HybridCloudApp/master/HybridCloudApp/Kubernetes/Backend/Network_Policies/permit_port_5050_rest_api_agent.yaml 3.2: Now try to referesh your Frontend App webpage. Does it work? Why? Note: Other command related to Network Policy that you may use - Display Kubernetes Network Policies - kubectl get NetworkPolicies Display Network Policy Details - kubectl describe NetworkPolicy Network Policy Name Delete Network Policy - kubectl delete NetworkPolicy Network Policy Name","title":"3. Apply Permit Port 5050 Network Policy:"}]}